{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda1e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (800, 600) to (800, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames generados.\n",
      "Creando video con imagen y audio...\n",
      "Video final con audio guardado en output.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vozyaudio import lee_audio, envolvente, track_pitch, espectro\n",
    "from scipy.signal import resample, correlate\n",
    "from imageio import get_writer\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# === CONFIGURACIÓN ===\n",
    "AUDIO_PATH = 'audios/music.wav'\n",
    "FPS = 60\n",
    "FRAME_FOLDER = 'frames'\n",
    "VIDEO_PATH = 'output.mp4'\n",
    "N_BARRAS = 60  # Número de barras del espectro\n",
    "\n",
    "# Crear carpeta\n",
    "os.makedirs(FRAME_FOLDER, exist_ok=True)\n",
    "\n",
    "# === 1. Cargar audio ===\n",
    "fs, x = lee_audio(AUDIO_PATH)\n",
    "x = x.astype(np.float32)\n",
    "dur = len(x) / fs\n",
    "n_frames = int(FPS * dur)\n",
    "samples_per_frame = int(fs / FPS)\n",
    "\n",
    "# === 2. Descriptores ===\n",
    "env = envolvente(x, fs=fs)\n",
    "pitch = track_pitch(x, fs)\n",
    "pitch = np.nan_to_num(pitch)\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# 1. Autocorrelación sobre la envolvente\n",
    "env_smooth = envolvente(x, fs=fs, tr=0.1)  # más estable\n",
    "corr_env = autocorrelacion(env_smooth)\n",
    "\n",
    "# 2. Estimar el tempo global\n",
    "min_lag = int(fs / 5)    # máx 5 Hz = 300 BPM\n",
    "max_lag = int(fs / 1.5)  # mín 1.5 Hz = 90 BPM\n",
    "lag_beat = np.argmax(corr_env[min_lag:max_lag]) + min_lag\n",
    "periodo_muestras = lag_beat\n",
    "\n",
    "# 3. Encontrar los picos en la envolvente\n",
    "peaks, _ = find_peaks(env_smooth, distance=periodo_muestras * 0.8)\n",
    "\n",
    "# Convertir los picos (en muestras) a tiempos (en segundos) y luego a frames\n",
    "beat_times = peaks / fs\n",
    "beat_frames = (beat_times * FPS).astype(int)\n",
    "\n",
    "def normalizar(v):\n",
    "    return (v - np.min(v)) / (np.max(v) - np.min(v) + 1e-9)\n",
    "\n",
    "def autocorrelacion(x_frame):\n",
    "    x_frame = x_frame - np.mean(x_frame)\n",
    "    corr = correlate(x_frame, x_frame, mode='full')\n",
    "    mid = len(corr) // 2\n",
    "    return corr[mid:] / np.max(np.abs(corr) + 1e-9)\n",
    "\n",
    "def detectar_ritmo(x_frame, fs, fmin=1.5, fmax=8):\n",
    "    corr = autocorrelacion(x_frame)\n",
    "    min_lag = int(fs / fmax)\n",
    "    max_lag = int(fs / fmin)\n",
    "    if max_lag >= len(corr): max_lag = len(corr) - 1\n",
    "    if min_lag >= max_lag: return 0.5, corr  # Valor por defecto\n",
    "    pico = np.argmax(corr[min_lag:max_lag]) + min_lag\n",
    "    periodo_seg = pico / fs\n",
    "    return periodo_seg, corr\n",
    "\n",
    "\n",
    "env = normalizar(env)\n",
    "pitch = normalizar(pitch)\n",
    "\n",
    "# Redimensionar descriptores al número de frames\n",
    "env_frame = np.interp(np.linspace(0, len(env), n_frames), np.arange(len(env)), env)\n",
    "pitch_frame = np.interp(np.linspace(0, len(pitch), n_frames), np.arange(len(pitch)), pitch)\n",
    "\n",
    "# === 3. Generar Frames ===\n",
    "print(\"Generando frames...\")\n",
    "for i in range(n_frames):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_facecolor((0, 0, 0))  # Fondo negro\n",
    "    \n",
    "    # Flash más visible en beat\n",
    "    if any(abs(i - bf) <= 2 for bf in beat_frames):  # mayor tolerancia\n",
    "        ax.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "\n",
    "    # ==== 3.1 Obtener trozo de señal actual ====\n",
    "    start = i * samples_per_frame\n",
    "    end = min(len(x), start + samples_per_frame)\n",
    "    x_frame = x[start:end]\n",
    "    \n",
    "    # Detectar ritmo\n",
    "    periodo, _ = detectar_ritmo(x_frame, fs)\n",
    "    t_actual = i / FPS\n",
    "    ritmo_osc = 0.5 * (1 + np.sin(2 * np.pi * t_actual / periodo))  # 0..1\n",
    "\n",
    "    # Efecto visual rítmico: círculo que late en el centro\n",
    "    ritmo_color = (ritmo_osc, 0.2, 1 - ritmo_osc)\n",
    "    ritmo_size = 300 * ritmo_osc + 20\n",
    "    ax.scatter(0.5, 0.5, s=ritmo_size, c=[ritmo_color], alpha=0.3)\n",
    "    \n",
    "    # ==== 3.2 Espectro (resample a N barras) ====\n",
    "    X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "    X_resampled = resample(X, N_BARRAS)\n",
    "    X_resampled = normalizar(X_resampled)\n",
    "\n",
    "    # ==== 3.3 Dibujar barras ====\n",
    "    bar_width = 1 / N_BARRAS\n",
    "    for j in range(N_BARRAS):\n",
    "        height = X_resampled[j]\n",
    "        ax.bar(j * bar_width, height, width=bar_width*0.8, color=(0.1, 0.8*height, 1.0), align='edge')\n",
    "\n",
    "    # ==== 3.4 Dibujar partícula ====\n",
    "    y_pos = pitch_frame[i]\n",
    "    size = 100 + env_frame[i] * 300\n",
    "    color = (1.0, env_frame[i], pitch_frame[i])\n",
    "    ax.scatter(0.5, y_pos, s=size, c=[color], alpha=0.8)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Frames generados.\")\n",
    "\n",
    "# === 4. Crear video ===\n",
    "\n",
    "def crear_video(output_path, fps=30, audio_path=AUDIO_PATH):\n",
    "    print(\"Creando video con imagen y audio...\")\n",
    "\n",
    "    temp_video_path = 'temp_video.mp4'\n",
    "\n",
    "    # Paso 1: crear video sin audio usando imageio\n",
    "    with get_writer(temp_video_path, fps=fps) as writer:\n",
    "        for i in range(n_frames):\n",
    "            frame_path = f\"{FRAME_FOLDER}/frame_{i:04d}.png\"\n",
    "            img = plt.imread(frame_path)\n",
    "            if img.dtype != np.uint8:\n",
    "                img = (img * 255).clip(0, 255).astype(np.uint8)  # conversión correcta\n",
    "            writer.append_data(img)\n",
    "\n",
    "    # Paso 2: combinar con audio usando ffmpeg\n",
    "    final_video_cmd = [\n",
    "        'ffmpeg',\n",
    "        '-y',\n",
    "        '-i', temp_video_path,\n",
    "        '-i', audio_path,\n",
    "        '-c:v', 'copy',\n",
    "        '-c:a', 'aac',\n",
    "        '-strict', 'experimental',\n",
    "        output_path\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(final_video_cmd, check=True)\n",
    "        print(f\"Video final con audio guardado en {output_path}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Error al unir audio con video usando ffmpeg.\")\n",
    "\n",
    "    # Limpiar temporal\n",
    "    if os.path.exists(temp_video_path):\n",
    "        os.remove(temp_video_path)\n",
    "\n",
    "crear_video(VIDEO_PATH, fps=FPS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
