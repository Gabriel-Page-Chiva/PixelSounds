{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3c3128",
   "metadata": {},
   "source": [
    "# PixelSounds. Visualizaci√≥n de audio en v√≠deo con Python\n",
    "### Introducci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8999096",
   "metadata": {},
   "source": [
    "En este caso, y siguiendo en la l√≠nea de los anteriores conversores propuestos en el proyecto PixelSounds, vamos a tratar de convertir audio a v√≠deo. Sin embargo, no va a ser cualquier tipo de v√≠deo. Vamos a tratar de generar un v√≠deo que nos permita visualizar en una primera vista algunas caracter√≠sticas del audio que hay detr√°s. Para ello vamos a extraer algunos descriptores del audio (los cuales especificaremos m√°s adelante) y vamos a transformarlos en algo visual. De esta forma, iremos generando fotogramas y mediante la libreria `ffmpeg` los uniremos para crear un video. En este cuaderno vamos a ir desgranando una por una las transformaciones que haremos para visualizar los distintos descriptores. Despu√©s uniremos todas las funciones para generar un fotograma. Tras aprender a generar un fotograma, podremos generar muchos para crear el v√≠deo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee9900",
   "metadata": {},
   "source": [
    "### Librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3991c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Instalando ffmpeg-python...\n",
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting future (from ffmpeg-python)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Installing collected packages: future, ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0 future-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts futurize.exe and pasteurize.exe are installed in 'c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "modules = {\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"matplotlib\": \"matplotlib\",\n",
    "    \"IPython\": \"ipywidgets\",\n",
    "    \"scipy\": \"scipy\",\n",
    "    \"librosa\": \"librosa\",\n",
    "    \"ipywidgets\": \"ipywidgets\",\n",
    "    \"bqplot\": \"bqplot\",\n",
    "    \"pyaudio\": \"pyaudio\",\n",
    "    \"ffmpeg\" : \"ffmpeg-python\"\n",
    "}\n",
    "\n",
    "for mod, pip_name in modules.items():\n",
    "    try:\n",
    "        __import__(mod)\n",
    "    except ImportError:\n",
    "        if pip_name:\n",
    "            print(f\"üîß Instalando {pip_name}...\")\n",
    "            %pip install {pip_name}\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è M√≥dulo {mod} no se puede instalar autom√°ticamente (builtin o personalizado)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46b1bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vozyaudio as vz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "# Instalar ffmpeg-python\n",
    "import ffmpeg\n",
    "import shutil\n",
    "import colorsys\n",
    "from vozyaudio import lee_audio, envolvente, track_pitch, espectro\n",
    "from scipy.signal import resample, correlate, find_peaks\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d627e05",
   "metadata": {},
   "source": [
    "### Los primeros pasos\n",
    "Antes de comenzar a hacer nada, vamos a definir algunos par√°metros b√°sicos que vamos a utilizar m√°s adelante. La configuraci√≥n del codificador consta de los siguientes par√°metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2d03b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existe: fotogramas/ ‚Äî nada que borrar.\n",
      "No existe: pruebas/ ‚Äî nada que borrar.\n",
      "Carpeta fotogramas creada.\n",
      "Carpeta pruebas creada.\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n\n",
    "AUDIO_PATH = 'audios/music.wav' # Ruta del audio que vamos a usar\n",
    "FPS = 25 # N√∫mero de fotogramas por segundo del v√≠deo resultante\n",
    "FRAME_FOLDER = 'fotogramas' # Ruta de la carpeta donde iremos guardando los fotogramas\n",
    "N_BARRAS = 60  # N√∫mero de barras del espectro\n",
    "\n",
    "# Borrar las carpetas de salida si existen\n",
    "for d in (\"fotogramas\", \"pruebas\"):\n",
    "    if os.path.exists(d):\n",
    "        print(f\"Eliminando carpeta existente: {d}/\")\n",
    "        shutil.rmtree(d)\n",
    "    else:\n",
    "        print(f\"No existe: {d}/ ‚Äî nada que borrar.\")\n",
    "\n",
    "# Crear carpetas de salida\n",
    "frames_dir = f\"fotogramas\"\n",
    "export_dir = f\"pruebas\"\n",
    "\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "print(f\"Carpeta {frames_dir} creada.\")\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "print(f\"Carpeta {export_dir} creada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b76af",
   "metadata": {},
   "source": [
    "Despu√©s de esto, cargaremos el audio y extraeremos su duraci√≥n, n√∫mero de muestras, y muestras por fotograma. Adem√°s, obtendremos el n√∫mero de frames que ha de tener el v√≠deo resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8161cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar audio\n",
    "fs, x = lee_audio(AUDIO_PATH) # Lee el audio\n",
    "x = x.astype(np.float32) # Transforma el tipo de dato\n",
    "dur = len(x) / fs # Duraci√≥n del audio\n",
    "n_frames = int(FPS * dur) # N√∫mero de frames del v√≠deo\n",
    "samples_per_frame = int(fs / FPS) # N√∫mero de muestras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83301fad",
   "metadata": {},
   "source": [
    "Tras hacer todo esto ya podemos empezar a generar las primeras componentes del fotograma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecc4b1",
   "metadata": {},
   "source": [
    "### C√≠rculo que se mueve con el pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead76ada",
   "metadata": {},
   "source": [
    "Lo primero que vamos a agregar al fotograma es un **circulo que se mueve de arriba a abajo con el pitch y cambia de tama√±o con la envolvente**. Para este paso, usaremos las funciones `envolvente` y `track_pitch` del m√≥dulo `vozyaudio` para extraer la envolvente y estimar el pitch del audio. Despu√©s normalizaremos ambos descriptores y los redimensionaremos para que se ajusten al n√∫mero de frames finales del video. Todo esto lo haremos dentro de la funci√≥n `obtener_descriptores` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(v):\n",
    "    \"\"\" Normaliza un vector al rango [0, 1].\n",
    "    Entrada:\n",
    "        v (numpy.ndarray): Vector de valores (por ejemplo, envolvente, pitch, espectro, etc.)\n",
    "    Salida:\n",
    "        v_norm (numpy.ndarray): Vector normalizado en el rango [0, 1]\n",
    "    \"\"\"\n",
    "    return (v - np.min(v)) / (np.max(v) - np.min(v) + 1e-9)\n",
    "\n",
    "def obtener_descriptores(x,fs):\n",
    "    \"\"\" Extrae distintos descriptores de la se√±al de audio.\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la se√±al\n",
    "        fs (int): Frecuencia de muestreo de la se√±al\n",
    "    Salida:\n",
    "        pitch_frame (numpy.ndarray): Vector con los valores de la estimaci√≥n del pitch normalizados y redimensionados al n√∫mero de frames\n",
    "        env_frame (numpy.ndarray): Vector con los valores de la envolvente normalizados y redimensionados al n√∫mero de frames \n",
    "    \"\"\"\n",
    "    # Extraemos los descriptores de envolvente y estimaci√≥n de pitch\n",
    "    env = envolvente(x, fs=fs) # Extraer envolvente\n",
    "    pitch = track_pitch(x, fs) # Estimar pitch\n",
    "    pitch = np.nan_to_num(pitch)  # Reemplaza NaNs por 0\n",
    "    \n",
    "    env = normalizar(env) # Normalizar ambos arrays\n",
    "    pitch = normalizar(pitch)\n",
    "    \n",
    "    # Redimensionar descriptores al n√∫mero de frames\n",
    "    env_frame = np.interp(np.linspace(0, len(env), n_frames), np.arange(len(env)), env)\n",
    "    pitch_frame = np.interp(np.linspace(0, len(pitch), n_frames), np.arange(len(pitch)), pitch)\n",
    "\n",
    "    return pitch_frame, env_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741604c9",
   "metadata": {},
   "source": [
    "Con estas variables ya tenemos todo lo necesario para generar el primer componente que variar√° durante el v√≠deo. Lo siguiente que debemos hacer es definir una funci√≥n que dibujo el circulo en pantalla variando seg√∫ne estos valores. A esta funci√≥n la llamaremos `dibujar_particula`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72bbcd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_particula(pitch, env):\n",
    "    \"\"\" Dibuja una part√≠cula que se mueve seg√∫n el pitch y cambia de tama√±o seg√∫n la envolvente del audio\n",
    "    Entrada:\n",
    "        pitch (float) : Valor de estimaci√≥n del pitch en un fotograma determinado\n",
    "        env (float) : Valor de la envolvente del audio en un fotograma determinado\n",
    "    Salida:\n",
    "        None: La funci√≥n solo dibuja en la figura y no devuelve nada\n",
    "    \"\"\"\n",
    "    y_pos = pitch\n",
    "    size = 100 + env * 300\n",
    "    color = (1.0, env, pitch)\n",
    "    plt.scatter(0.5, y_pos, s=size, c=[color], alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a76a4",
   "metadata": {},
   "source": [
    "Para visualizar que el resultado es el que esperamos tenemos que definir una funci√≥n b√°sica de generaci√≥n de frames. Esta funci√≥n la iremos ampliando a medida que vayamos a√±adiendo m√°s componentes al fotograma. Cuando tengamos todos los fotogramas generados los uniremos mediante la funci√≥n `crear_video`  para ver el resultado. Esta funci√≥n hace uso de `subprocess` y de `ffmpeg` para crear el video de manera r√°pida simplemente pas√°ndole el archivo `generarVideo.bat` que se encargar√° de ejecutar los comandos ffmpeg. La usaremos mucho a lo largo del cuaderno. De momento, probemos a generar los fotogramas con solo esta componente y ver el video resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e500a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el v√≠deo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la se√±al\n",
    "        fs (int): Frecuencia de muestreo de la se√±al\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "        # Visual: C√≠rculo que sube/baja con pitch y cambia tama√±o con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "out = \"pruebas/pruebaPitch.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8440c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_video(audio_path,out):\n",
    "    try:\n",
    "        if os.path.exists(out):\n",
    "            os.remove(out)\n",
    "        subprocess.run(\n",
    "        [\"cmd\", \"/c\", \"generarVideo.bat\", audio_path, out],\n",
    "        check=True\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error al ejecutar generarVideo.bat:\", e)\n",
    "    finally:\n",
    "        # shutil.rmtree('fotogramas/')\n",
    "        print('\\nProcesado terminado.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f953c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al ejecutar generarVideo.bat: Command '['cmd', '/c', 'generarVideo.bat', 'audios/music.wav', 'pruebas/pruebaPitch.mp4']' returned non-zero exit status 1.\n",
      "\n",
      "Procesado terminado.\n"
     ]
    }
   ],
   "source": [
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe1ab6",
   "metadata": {},
   "source": [
    "A continuaci√≥n visualizaremos el resultado en la siguiente celda. Podemos ver como la posici√≥n vertical varia con el pitch y la intensidad del halo varia con la envolvente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3c5e926",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "To embed videos, you must pass embed=True (this may make your notebook files huge)\nConsider passing Video(url='...')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mVideo\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpruebas/pruebaPitch.mp4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\display.py:1217\u001b[39m, in \u001b[36mVideo.__init__\u001b[39m\u001b[34m(self, data, url, filename, embed, mimetype, width, height, html_attributes)\u001b[39m\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embed:\n\u001b[32m   1212\u001b[39m     msg = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join([\n\u001b[32m   1213\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo embed videos, you must pass embed=True \u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1214\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(this may make your notebook files huge)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConsider passing Video(url=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1216\u001b[39m     ])\n\u001b[32m-> \u001b[39m\u001b[32m1217\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1219\u001b[39m \u001b[38;5;28mself\u001b[39m.mimetype = mimetype\n\u001b[32m   1220\u001b[39m \u001b[38;5;28mself\u001b[39m.embed = embed\n",
      "\u001b[31mValueError\u001b[39m: To embed videos, you must pass embed=True (this may make your notebook files huge)\nConsider passing Video(url='...')"
     ]
    }
   ],
   "source": [
    "Video(\"pruebas/pruebaPitch.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d32c59",
   "metadata": {},
   "source": [
    "### Barras espectrales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7d302",
   "metadata": {},
   "source": [
    "Despu√©s de hacer que la estimaci√≥n del pitch sea visible en el video vamos a a√±adir alg√∫n componente que nos muestre de alguna forma la **energ√≠a de cada banda de frecuencia en la se√±al**. Para ello vamos a usar barras espectrales que aumenten y disminuyan en funci√≥n de la energ√≠a espectral de la se√±al. Para ello haremos uso de la funci√≥n `espectro`. Lo que vamos a hacer es dividir la se√±al en trozos. De cada trozo extraeremos su informaci√≥n espectral y las adapataremos al componente visual de la barras. Todo esto lo introduciremos dentro de la funci√≥n `generar_frames`.\n",
    "\n",
    "La primera funci√≥n que vamos a desarrollar en este bloque es la de `dibujar_barras`. Se encargar√° de plotear las barras en los fotogramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c6fe050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_barras(X_resampled, N_BARRAS):\n",
    "    \"\"\" Dibuja barras verticales que representan la energ√≠a en diferentes bandas de frecuencia.\n",
    "    Entrada:\n",
    "        X_resampled (numpy.ndarray): Vector con las amplitudes espectrales reescaladas a N_BARRAS bandas\n",
    "        N_BARRAS (int): N√∫mero total de barras a dibujar\n",
    "    Salida:\n",
    "        None: Las barras se dibujan directamente sobre la figura\n",
    "    \"\"\"\n",
    "    bar_width = 1 / N_BARRAS\n",
    "    for j in range(N_BARRAS):\n",
    "        height = X_resampled[j]\n",
    "        color = (0.1, 0.8 * height, 1.0)\n",
    "        plt.bar(j * bar_width, height, width=bar_width*0.8, color=color, align='edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359458d",
   "metadata": {},
   "source": [
    "Tras esto, vamos a a√±adir a `generar_frames` la parte de dividir la se√±al en trozos, extraer su espectro y dibujar las barras espectrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94ed8cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %\n",
      "Procesado terminado.\n"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el v√≠deo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la se√±al\n",
    "        fs (int): Frecuencia de muestreo de la se√±al\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        # ------NUEVO--------\n",
    "        \n",
    "        #  Obtener trozo de se√±al actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # ------NUEVO--------\n",
    "\n",
    "        # Visual: C√≠rculo que sube/baja con pitch y cambia tama√±o con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"pruebas/pruebaBarras.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f459226",
   "metadata": {},
   "source": [
    "En la siguiente celda podemos observar como las barras verticales varian con la energ√≠a de la se√±al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01d156d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"pruebas/pruebaBarras.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"pruebas/pruebaBarras.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f6dd82",
   "metadata": {},
   "source": [
    "### Autocorrelaci√≥n y patrones r√≠tmicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b3968",
   "metadata": {},
   "source": [
    "La autocorrelaci√≥n mide c√≥mo una se√±al se parece a s√≠ misma desplazada en el tiempo. En m√∫sica, eso se traduce en:\n",
    "\n",
    "* Picos peri√≥dicos en la autocorrelaci√≥n = ritmo repetitivo o beats.\n",
    "\n",
    "* Puede ayudarte a detectar tempo, pulsos o patrones repetitivos como los que tienen bases de bater√≠a, loops, etc.\n",
    "\n",
    "Por esto sabemos que la autocorrelaci√≥n es una herramienta realmente potente para analizar el contenido r√≠tmico de un audio. Lo primero que vamos a hacer es definir una funci√≥n que nos ayude a calcular la autocorrelaci√≥n de una se√±al de audio. Para ello nos ayudaremos de la funci√≥n `correlate` del m√≥dulo `scipy.signal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ab720dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelacion(x_frame):\n",
    "    \"\"\" Calcula la autocorrelaci√≥n normalizada de una ventana de se√±al de audio.\n",
    "    Entrada:\n",
    "        x_frame (numpy.ndarray): Fragmento de se√±al de audio (ventana temporal)\n",
    "    Salida:\n",
    "        corr_norm (numpy.ndarray): Autocorrelaci√≥n normalizada desde el retardo cero hacia adelante\n",
    "    \"\"\"\n",
    "    x_frame = x_frame - np.mean(x_frame)\n",
    "    corr = correlate(x_frame, x_frame, mode='full')\n",
    "    mid = len(corr) // 2\n",
    "    return corr[mid:] / np.max(np.abs(corr) + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936bc2f",
   "metadata": {},
   "source": [
    "Una vez hecho esto podemos crear una funci√≥n `detectar_ritmo` que estime el ritmo de un fragmento de audio usando la correlaci√≥n, y con este ritmo podemos crear un **c√≠rculo en el centro del frame que lata con intensidad variante seg√∫n el ritmo**. Para ello utilizaremos `sin(2œÄ * t / periodo)`. Con `detectar_ritmo` tenemos todo lo necesario para crear la funci√≥n `dibujar_circulo_ritmico`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "317f25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_ritmo(x_frame, fs, fmin=1.5, fmax=8):\n",
    "    \"\"\" Estima el periodo r√≠tmico de un fragmento de audio mediante autocorrelaci√≥n.\n",
    "    Entrada:\n",
    "        x_frame (numpy.ndarray): Fragmento de se√±al de audio (ventana temporal)\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "        fmin (float): Frecuencia m√≠nima esperada del ritmo (en Hz)\n",
    "        fmax (float): Frecuencia m√°xima esperada del ritmo (en Hz)\n",
    "    Salida:\n",
    "        periodo_seg (float): Periodo estimado del ritmo en segundos\n",
    "        corr (numpy.ndarray): Autocorrelaci√≥n normalizada del fragmento de audio\n",
    "    \"\"\"\n",
    "    corr = autocorrelacion(x_frame)\n",
    "    min_lag = int(fs / fmax)\n",
    "    max_lag = int(fs / fmin)\n",
    "    if max_lag >= len(corr): max_lag = len(corr) - 1\n",
    "    if min_lag >= max_lag: return 0.5, corr  # Valor por defecto\n",
    "    pico = np.argmax(corr[min_lag:max_lag]) + min_lag\n",
    "    periodo_seg = pico / fs\n",
    "    return periodo_seg, corr\n",
    "\n",
    "def dibujar_circulo_ritmico(t_actual, periodo):\n",
    "    \"\"\" Dibuja un c√≠rculo que pulsa r√≠tmicamente en el centro del frame seg√∫n un periodo dado.\n",
    "    Entrada:\n",
    "        t_actual (float): Tiempo actual del video en segundos\n",
    "        periodo (float): Periodo r√≠tmico estimado en segundos\n",
    "    Salida:\n",
    "        None: El c√≠rculo se dibuja directamente sobre la figura\n",
    "    \"\"\"\n",
    "    ritmo_osc = 0.5 * (1 + np.sin(2 * np.pi * t_actual / periodo))\n",
    "    color = (ritmo_osc, 0.2, 1 - ritmo_osc)\n",
    "    size = 300 * ritmo_osc + 20\n",
    "    plt.scatter(0.5, 0.5, s=size, c=[color], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b6602",
   "metadata": {},
   "source": [
    "Ahora a√±adiremos toda esta informaci√≥n al bucle de generaci√≥n del fotograma. Aprovecharemos la divisi√≥n a trozos de la se√±al que hemos implementado antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2edc938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %\n",
      "Procesado terminado.\n"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el v√≠deo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la se√±al\n",
    "        fs (int): Frecuencia de muestreo de la se√±al\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        #  Obtener trozo de se√±al actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # ------NUEVO------\n",
    "        \n",
    "        # Detecci√≥n r√≠tmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "        \n",
    "         # ------NUEVO------\n",
    "\n",
    "        # Visual: C√≠rculo que sube/baja con pitch y cambia tama√±o con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"pruebas/pruebaRitmo.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19952894",
   "metadata": {},
   "source": [
    "Ahora visualizaremos el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11cdf852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"pruebas/pruebaRitmo.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"pruebas/pruebaRitmo.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e09a91",
   "metadata": {},
   "source": [
    "Podemos observar como el circulo late de forma constante al ritmo constante delimitado por los beats de fondo del audio original. Sin embargo, este dise√±o que acabamos de crear tiene **una debilidad importante**, y es que no es capaz de distinguir qu√© ritmo del audio tiene contenido arm√≥nico relevante para nosotros. Debido a que estamos aplicando la autocorrelaci√≥n directamente sobre la onda real podemos estar obteniendo partes del ritmo que son poco relevantes en arm√≥nicamente en la canci√≥n.\n",
    "\n",
    "Por ello, para hacer un aislamiento de estas partes poco relevantes y quedarnos con lo que nos interesa de verdad (y en consecuencia hacer que el circulo lata todav√≠a m√°s acorde con la canci√≥n), **vamos a aplicar la autocorrelaci√≥n sobre la envolvente**. De esta forma vamos a obtener los **golpes r√≠tmicos reales** de la se√±al. Por cada golpe r√≠tmico vamos a generar un flash en el centro del fotograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ca0fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_frames(x, fs):\n",
    "    \"\"\" Detecta los beats del audio a partir de la envolvente y devuelve sus ubicaciones en frames.\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Se√±al de audio completa\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "    Salida:\n",
    "        beat_frames (numpy.ndarray): √çndices de frame donde se detectan beats en la se√±al\n",
    "    \"\"\"\n",
    "    # Autocorrelaci√≥n sobre la envolvente\n",
    "    env_smooth = envolvente(x, fs=fs, tr=0.1)  # m√°s estable\n",
    "    corr_env = autocorrelacion(env_smooth)\n",
    "\n",
    "    # Estimar el tempo global\n",
    "    min_lag = int(fs / 5)    # m√°x 5 Hz = 300 BPM\n",
    "    max_lag = int(fs / 1.5)  # m√≠n 1.5 Hz = 90 BPM\n",
    "    lag_beat = np.argmax(corr_env[min_lag:max_lag]) + min_lag\n",
    "    periodo_muestras = lag_beat\n",
    "\n",
    "    # Encontrar los picos en la envolvente\n",
    "    peaks, _ = find_peaks(env_smooth, distance=periodo_muestras * 0.8)\n",
    "\n",
    "    # Convertir los picos (en muestras) a tiempos (en segundos) y luego a frames\n",
    "    beat_times = peaks / fs\n",
    "    beat_frames = (beat_times * FPS).astype(int)\n",
    "    return beat_frames\n",
    "\n",
    "def es_beat(i, beat_frames, tolerancia=2):\n",
    "    \"\"\" Determina si un frame est√° dentro de los limites de un beat detectado.\n",
    "    Entrada:\n",
    "        frame_index (int): √çndice del frame actual en el video\n",
    "        beat_frames (list of int): Lista de frames donde se detectaron beats\n",
    "        tolerancia (int): N√∫mero de frames de margen alrededor de cada beat\n",
    "    Salida:\n",
    "        es_beat (bool): True si el frame est√° cerca de un beat, False en caso contrario\n",
    "    \"\"\"\n",
    "    return any(abs(i - bf) <= tolerancia for bf in beat_frames)\n",
    "\n",
    "\n",
    "def dibujar_flash(i, beats):\n",
    "    \"\"\" Dibuja un flash visual en el centro del frame, usado para resaltar beats detectados.\n",
    "    Entrada:\n",
    "        beats (numpy.ndarray): Array con los √≠ndices de los fotogramas donde hay un beat\n",
    "    Salida:\n",
    "        None: El flash se dibuja directamente sobre la figura\n",
    "    \"\"\"\n",
    "    # Halo pulsante animado en el centro tras beat\n",
    "    for bf in beats:\n",
    "        frames_from_beat = i - bf\n",
    "        if 0 <= frames_from_beat <= 4:  # duraci√≥n 4 frames\n",
    "            grow = 1 - frames_from_beat / 4\n",
    "            size = 2000 * grow\n",
    "            alpha = 0.8 * grow\n",
    "            plt.scatter(0.5, 0.5, s=size, c='magenta', alpha=alpha, edgecolors='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d034d",
   "metadata": {},
   "source": [
    "Ahora vamos a ampliar la funci√≥n `generar_frames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7715b1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %\n",
      "Procesado terminado.\n"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el v√≠deo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la se√±al\n",
    "        fs (int): Frecuencia de muestreo de la se√±al\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash m√°s visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #------NUEVO------\n",
    "            \n",
    "        #  Obtener trozo de se√±al actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # Detecci√≥n r√≠tmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: C√≠rculo que sube/baja con pitch y cambia tama√±o con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"pruebas/pruebaRitmo2.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fec458ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"pruebas/pruebaRitmo2.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"pruebas/pruebaRitmo2.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471210a",
   "metadata": {},
   "source": [
    "Como vemos, cada vez tarda m√°s en generar los fotogramos puesto que estamso todo el rato a√±adiendo nueva a informaci√≥n a la funci√ßon `generar_frames`. Con este √∫ltimo componente en el v√≠deo ya podemos dar por finalizado todo el proceso de este conversor de audio a v√≠deo. Sin embargo, nos gustar√≠a a√±adir un √∫ltimo apartado con algo que nos parece curioso pero que no queremos incluir en la versi√≥n final ya que el resultado es bastante epileptico y la idea est√° sacada de Internet. Lo dejamos como celda opcional a ejecutar. Si no quieres hacerlo, salta directamente al apartado del resultado final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c16bca",
   "metadata": {},
   "source": [
    "### Opcional: variaci√≥n de color de fondo con centroide espectral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade7257",
   "metadata": {},
   "source": [
    "Para darle un toque final al programa est√°bamos pensando en jugar con la tonalidad del audio. Queriamos encontrar algo que variase seg√∫n los cambios de tonalidad (cambio de grave a agudo etc). Se nos ocurri√≥ que podr√≠amos variar el color de fondo y, buscando por Internet, encontramos algo que podemos obtener a partir del espectro de la se√±al: **el centroide espectral**. El centroide espectral indica la \"brillantez\" del sonido, es decir, qu√© tan concentrada est√° la energ√≠a en frecuencias altas.\n",
    "\n",
    "* Un centroide alto ‚Üí sonidos agudos o brillantes ‚Üí fondo m√°s claro o c√°lido.\n",
    "\n",
    "* Un centroide bajo ‚Üí sonidos graves o apagados ‚Üí fondo m√°s oscuro o fr√≠o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "from vozyaudio import espectro\n",
    "import numpy as np\n",
    "\n",
    "import colorsys\n",
    "from vozyaudio import espectro\n",
    "import numpy as np\n",
    "\n",
    "def color_fondo_por_centroide(X, fa, x_frame, fs):\n",
    "    \"\"\" Genera un color RGB para el fondo en funci√≥n del centroide espectral del frame.\n",
    "    Entrada:\n",
    "        X (np.ndarray): M√≥dulo del espectro del frame de audio (magnitudes)\n",
    "        fa (np.ndarray): Vector de frecuencias correspondientes al espectro\n",
    "        x_frame (np.ndarray): Fragmento de se√±al de audio correspondiente al frame\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "    Salida:\n",
    "        fondo_color (tuple): Color RGB normalizado (0-1) para usar como fondo del frame\n",
    "    \"\"\"\n",
    "    # Evitar errores si X est√° vac√≠o\n",
    "    if np.sum(X) == 0 or len(fa) != len(X):\n",
    "        return (0, 0, 0.1)  # fondo oscuro por defecto\n",
    "\n",
    "    # Centroide espectral\n",
    "    centroide = np.sum(fa * X) / (np.sum(X) + 1e-9)\n",
    "\n",
    "    # Normalizar centroide al rango 0‚Äì1 basado en un rango realista (0‚Äì4000 Hz)\n",
    "    centroide_norm = np.clip(centroide / 4000, 0, 1)\n",
    "\n",
    "    # Usar centroide como matiz, pero tambi√©n afectar brillo\n",
    "    h = centroide_norm                   # matiz (rojo ‚Üî azul)\n",
    "    s = 0.9                              # saturaci√≥n constante\n",
    "    v = 0.3 + 0.7 * centroide_norm       # m√°s agudo ‚Üí m√°s brillante\n",
    "\n",
    "    fondo_color = colorsys.hsv_to_rgb(h, s, v)\n",
    "    return fondo_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79de99",
   "metadata": {},
   "source": [
    "Con esta funci√≥n, la funci√≥n `generar_frames` quedar√≠a as√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85257f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el v√≠deo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la se√±al\n",
    "        fs (int): Frecuencia de muestreo de la se√±al\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash m√°s visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #  Obtener trozo de se√±al actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        fondo_color =  color_fondo_por_centroide(X,fa,x_frame,fs)\n",
    "        fig.set_facecolor(fondo_color)\n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        # Detecci√≥n r√≠tmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: C√≠rculo que sube/baja con pitch y cambia tama√±o con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"pruebas/pruebaColor.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"pruebas/pruebaColor.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3be13d",
   "metadata": {},
   "source": [
    "### Resultado final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bbe5d",
   "metadata": {},
   "source": [
    "Para finalizar, vamos a dejar una celda con la funci√≥n final de `generar_frames` y las rutas para poder cambiar f√°cilmente los audios y poder testear audios distintos al de ejemplo. Recomendamos testear con audios de duraci√≥n no superior a 15 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3106348a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %\n",
      "Procesado terminado.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"final.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuraci√≥n\n",
    "AUDIO_PATH = 'audios/music.wav' # Ruta del audio que vamos a usar\n",
    "FPS = 25 # N√∫mero de fotogramas por segundo del v√≠deo resultante\n",
    "FRAME_FOLDER = 'fotogramas' # Ruta de la carpeta donde iremos guardando los fotogramas\n",
    "N_BARRAS = 60  # N√∫mero de barras del espectro\n",
    "\n",
    "# Cargar audio\n",
    "fs, x = lee_audio(AUDIO_PATH) # Lee el audio\n",
    "x = x.astype(np.float32) # Transforma el tipo de dato\n",
    "dur = len(x) / fs # Duraci√≥n del audio\n",
    "n_frames = int(FPS * dur) # N√∫mero de frames del v√≠deo\n",
    "samples_per_frame = int(fs / FPS) # N√∫mero de muestras \n",
    "\n",
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el v√≠deo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la se√±al\n",
    "        fs (int): Frecuencia de muestreo de la se√±al\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash m√°s visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #  Obtener trozo de se√±al actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # Detecci√≥n r√≠tmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: C√≠rculo que sube/baja con pitch y cambia tama√±o con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"final.mp4\"\n",
    "crear_video(AUDIO_PATH,out)\n",
    "Video(\"final.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99b5ac",
   "metadata": {},
   "source": [
    "### Resumen de PixelSounds: visualizaci√≥n de audio en video con Python\n",
    "\n",
    "#### Objetivo general\n",
    "\n",
    "Nuestro objetivo principal es el de crear un video din√°mico a partir de un archivo de audio, donde la visualizaci√≥n reaccione a distintos descriptores del sonido como ritmo, espectro, envolvente, etc., utilizando exclusivamente Python.\n",
    "\n",
    "#### Resumen de lo aprendido y conceptos explicados\n",
    "\n",
    "##### Descriptores de audio\n",
    "Hemos sabido comprender y aplicar diferentes descriptores que capturan distintas propiedades del sonido:\n",
    "\n",
    "1. Envolvente de amplitud\n",
    "\n",
    "* Representa la variaci√≥n de la energ√≠a de la se√±al a lo largo del tiempo.\n",
    "\n",
    "* √ötil para detectar intensidad, ataques o silencios.\n",
    "\n",
    "2. Espectro de frecuencias\n",
    "\n",
    "* Se obtiene mediante la Transformada de Fourier.\n",
    "\n",
    "* Permite ver qu√© frecuencias est√°n presentes en un instante.\n",
    "\n",
    "3. Centroide espectral\n",
    "\n",
    "* Indica el ‚Äúcentro de gravedad‚Äù del espectro.\n",
    "\n",
    "* Cuanto m√°s alto, m√°s brillante o agudo se percibe el sonido.\n",
    "\n",
    "4. Autocorrelaci√≥n\n",
    "\n",
    "* Herramienta para detectar periodicidad o repetici√≥n.\n",
    "\n",
    "* Aplicada sobre la envolvente para estimar el ritmo o tempo.\n",
    "\n",
    "5. Detecci√≥n de beats\n",
    "\n",
    "* Se basa en picos peri√≥dicos de energ√≠a detectados en la envolvente.\n",
    "\n",
    "* Permite sincronizar efectos visuales con los golpes musicales.\n",
    "\n",
    "##### Procesamiento por fotogramas\n",
    "* El audio se divide en ventanas temporales por fotogramas de v√≠deo.\n",
    "\n",
    "* Cada fragmento se analiza individualmente para obtener descriptores y generar visuales sincronizados.\n",
    "\n",
    "##### Relaci√≥n audio ‚Üí imagen\n",
    "Hemos aprendido a convertir propiedades del audio en par√°metros visuales:\n",
    "\n",
    "* Pitch o frecuencia ‚Üí posici√≥n vertical.\n",
    "\n",
    "* Envolvente ‚Üí tama√±o o brillo.\n",
    "\n",
    "* Centroide espectral ‚Üí color.\n",
    "\n",
    "* Beat ‚Üí efectos puntuales (flashes, cambios bruscos).\n",
    "\n",
    "Hemos aplicado conceptos de visualizaci√≥n din√°mica, donde el v√≠deo no es est√°tico, sino que evoluciona en funci√≥n del sonido.\n",
    "\n",
    "##### Programaci√≥n y dise√±o de sistema\n",
    "* Hemos optado por un dise√±o modular del sistema: funciones peque√±as y reutilizables para an√°lisis, extracci√≥n de descriptores y visualizaci√≥n.\n",
    "\n",
    "* Hemos hecho uso de librer√≠as como matplotlib, scipy, numpy, colorsys.\n",
    "\n",
    "* Hemos realizado la automatizaci√≥n de video + audio con ffmpeg.\n",
    "\n",
    "#### Funciones creadas\n",
    "\n",
    "* `normalizar(v)`: normaliza valores en rango [0, 1].\n",
    "\n",
    "* `autocorrelacion(x_frame)`: autocorrelaci√≥n normalizada de un fragmento.\n",
    "\n",
    "* `detectar_ritmo(x_frame, fs, ...)`: calcula el periodo dominante.\n",
    "\n",
    "* `es_beat(frame_index, beat_frames, ...)`: determina si un frame es un beat.\n",
    "\n",
    "* `dibujar_flash()`: dibuja un flash central.\n",
    "\n",
    "* `dibujar_barras(ax, X_resampled, N_BARRAS)`: visualizaci√≥n espectral.\n",
    "\n",
    "* `color_fondo_por_centroide(x_frame, fs)`: color de fondo basado en centroide.\n",
    "\n",
    "* `beat_frames(x, fs)`: calcula los beats a partir de la envolvente.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
