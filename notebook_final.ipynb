{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3c3128",
   "metadata": {},
   "source": [
    "# PIXELSOUNDS\n",
    "## Codificador de audio a vídeo\n",
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8999096",
   "metadata": {},
   "source": [
    "En este caso, y siguiendo en la línea de los anteriores conversores propuestos en el proyecto PixelSounds, vamos a tratar de convertir audio a vídeo. Sin embargo, no va a ser cualquier tipo de vídeo. Vamos a tratar de generar un vídeo que nos permita visualizar en una primera vista algunas características del audio que hay detrás. Para ello vamos a extraer algunos descriptores del audio (los cuales especificaremos más adelante) y vamos a transformarlos en algo visual. De esta forma, iremos generando fotogramas y mediante la libreria `ffmpeg` los uniremos para crear un video. En este cuaderno vamos a ir desgranando una por una las transformaciones que haremos para visualizar los distintos descriptores. Después uniremos todas las funciones para generar un fotograma. Tras aprender a generar un fotograma, podremos generar muchos para crear el vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee9900",
   "metadata": {},
   "source": [
    "### Librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b1bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vozyaudio as vz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import colorsys\n",
    "from vozyaudio import lee_audio, envolvente, track_pitch, espectro\n",
    "from scipy.signal import resample, correlate, find_peaks\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d627e05",
   "metadata": {},
   "source": [
    "### Los primeros pasos\n",
    "Antes de comenzar a hacer nada, vamos a definir algunos parámetros básicos que vamos a utilizar más adelante. La configuración del codificador consta de los siguientes parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "AUDIO_PATH = 'audios/music.wav' # Ruta del audio que vamos a usar\n",
    "FPS = 25 # Número de fotogramas por segundo del vídeo resultante\n",
    "FRAME_FOLDER = 'fotogramas' # Ruta de la carpeta donde iremos guardando los fotogramas\n",
    "N_BARRAS = 60  # Número de barras del espectro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b76af",
   "metadata": {},
   "source": [
    "Después de esto, cargaremos el audio y extraeremos su duración, número de muestras, y muestras por fotograma. Además, obtendremos el número de frames que ha de tener el vídeo resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar audio\n",
    "fs, x = lee_audio(AUDIO_PATH) # Lee el audio\n",
    "x = x.astype(np.float32) # Transforma el tipo de dato\n",
    "dur = len(x) / fs # Duración del audio\n",
    "n_frames = int(FPS * dur) # Número de frames del vídeo\n",
    "samples_per_frame = int(fs / FPS) # Número de muestras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83301fad",
   "metadata": {},
   "source": [
    "Tras hacer todo esto ya podemos empezar a generar las primeras componentes del fotograma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecc4b1",
   "metadata": {},
   "source": [
    "### Círculo que se mueve con el pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead76ada",
   "metadata": {},
   "source": [
    "Lo primero que vamos a agregar al fotograma es un **circulo que se mueve de arriba a abajo con el pitch y cambia de tamaño con la envolvente**. Para este paso, usaremos las funciones `envolvente` y `track_pitch` del módulo `vozyaudio` para extraer la envolvente y estimar el pitch del audio. Después normalizaremos ambos descriptores y los redimensionaremos para que se ajusten al número de frames finales del video. Todo esto lo haremos dentro de la función `obtener_descriptores` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(v):\n",
    "    \"\"\" Normaliza un vector al rango [0, 1].\n",
    "    Entrada:\n",
    "        v (numpy.ndarray): Vector de valores (por ejemplo, envolvente, pitch, espectro, etc.)\n",
    "    Salida:\n",
    "        v_norm (numpy.ndarray): Vector normalizado en el rango [0, 1]\n",
    "    \"\"\"\n",
    "    return (v - np.min(v)) / (np.max(v) - np.min(v) + 1e-9)\n",
    "\n",
    "def obtener_descriptores(x,fs):\n",
    "    \"\"\" Extrae distintos descriptores de la señal de audio.\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "    Salida:\n",
    "        pitch_frame (numpy.ndarray): Vector con los valores de la estimación del pitch normalizados y redimensionados al número de frames\n",
    "        env_frame (numpy.ndarray): Vector con los valores de la envolvente normalizados y redimensionados al número de frames\n",
    "        \n",
    "    \"\"\"\n",
    "    # Extraemos los descriptores de envolvente y estimación de pitch\n",
    "    env = envolvente(x, fs=fs) # Extraer envolvente\n",
    "    pitch = track_pitch(x, fs) # Estimar pitch\n",
    "    pitch = np.nan_to_num(pitch)  # Reemplaza NaNs por 0\n",
    "    \n",
    "    env = normalizar(env) # Normalizar ambos arrays\n",
    "    pitch = normalizar(pitch)\n",
    "    \n",
    "    # Redimensionar descriptores al número de frames\n",
    "    env_frame = np.interp(np.linspace(0, len(env), n_frames), np.arange(len(env)), env)\n",
    "    pitch_frame = np.interp(np.linspace(0, len(pitch), n_frames), np.arange(len(pitch)), pitch)\n",
    "\n",
    "    return pitch_frame, env_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741604c9",
   "metadata": {},
   "source": [
    "Con estas variables ya tenemos todo lo necesario para generar el primer componente que variará durante el vídeo. Lo siguiente que debemos hacer es definir una función que dibujo el circulo en pantalla variando segúne estos valores. A esta función la llamaremos `dibujar_particula`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbcd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_particula(pitch, env):\n",
    "    \"\"\" Dibuja una partícula que se mueve según el pitch y cambia de tamaño según la envolvente del audio\n",
    "    Entrada:\n",
    "        pitch (float) : Valor de estimación del pitch en un fotograma determinado\n",
    "        env (float) : Valor de la envolvente del audio en un fotograma determinado\n",
    "    Salida:\n",
    "        None: La función solo dibuja en la figura y no devuelve nada\n",
    "    \"\"\"\n",
    "    y_pos = pitch\n",
    "    size = 100 + env * 300\n",
    "    color = (1.0, env, pitch)\n",
    "    plt.scatter(0.5, y_pos, s=size, c=[color], alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a76a4",
   "metadata": {},
   "source": [
    "Para visualizar que el resultado es el que esperamos tenemos que definir una función básica de generación de frames. Esta función la iremos ampliando a medida que vayamos añadiendo más componentes al fotograma. Cuando tengamos todos los fotogramas generados los uniremos mediante la función `crear_video`  para ver el resultado. Esta función hace uso de `subprocess` y de `ffmpeg` para crear el video de manera rápida simplemente pasándole el archivo `generarVideo.bat` que se encargará de ejecutar los comandos ffmpeg. La usaremos mucho a lo largo del cuaderno. De momento, probemos a generar los fotogramas con solo esta componente y ver el video resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8440c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"pruebas/pruebaPitch.mp4\"\n",
    "def crear_video(audio_path,out):\n",
    "    try:\n",
    "        if os.path.exists(out):\n",
    "            os.remove(out)\n",
    "        subprocess.run(\n",
    "        [\"cmd\", \"/c\", \"generarVideo.bat\", audio_path, out],\n",
    "        check=True\n",
    "        )\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error al ejecutar generarVideo.bat:\", e)\n",
    "    finally:\n",
    "        # shutil.rmtree('fotogramas/')\n",
    "        print('\\nProcesado terminado.')\n",
    "\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe1ab6",
   "metadata": {},
   "source": [
    "A continuación visualizaremos el resultado en la siguiente celda. Podemos ver como la posición vertical varia con el pitch y la intensidad del halo varia con la envolvente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"pruebas/pruebaPitch.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d32c59",
   "metadata": {},
   "source": [
    "### Barras espectrales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7d302",
   "metadata": {},
   "source": [
    "Después de hacer que la estimación del pitch sea visible en el video vamos a añadir algún componente que nos muestre de alguna forma la **energía de cada banda de frecuencia en la señal**. Para ello vamos a usar barras espectrales que aumenten y disminuyan en función de la energía espectral de la señal. Para ello haremos uso de la función `espectro`. Lo que vamos a hacer es dividir la señal en trozos. De cada trozo extraeremos su información espectral y las adapataremos al componente visual de la barras. Todo esto lo introduciremos dentro de la función `generar_frames`.\n",
    "\n",
    "La primera función que vamos a desarrollar en este bloque es la de `dibujar_barras`. Se encargará de plotear las barras en los fotogramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6fe050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_barras(X_resampled, N_BARRAS):\n",
    "    \"\"\" Dibuja barras verticales que representan la energía en diferentes bandas de frecuencia.\n",
    "    Entrada:\n",
    "        X_resampled (numpy.ndarray): Vector con las amplitudes espectrales reescaladas a N_BARRAS bandas\n",
    "        N_BARRAS (int): Número total de barras a dibujar\n",
    "    Salida:\n",
    "        None: Las barras se dibujan directamente sobre la figura\n",
    "    \"\"\"\n",
    "    bar_width = 1 / N_BARRAS\n",
    "    for j in range(N_BARRAS):\n",
    "        height = X_resampled[j]\n",
    "        color = (0.1, 0.8 * height, 1.0)\n",
    "        plt.bar(j * bar_width, height, width=bar_width*0.8, color=color, align='edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359458d",
   "metadata": {},
   "source": [
    "Tras esto, vamos a añadir a `generar_frames` la parte de dividir la señal en trozos, extraer su espectro y dibujar las barras espectrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        # ------NUEVO--------\n",
    "        \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # ------NUEVO--------\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"pruebas/pruebaBarras.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f459226",
   "metadata": {},
   "source": [
    "En la siguiente celda podemos observar como las barras verticales varian con la energía de la señal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d156d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"pruebas/pruebaBarras.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f6dd82",
   "metadata": {},
   "source": [
    "### Autocorrelación y patrones rítmicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b3968",
   "metadata": {},
   "source": [
    "La autocorrelación mide cómo una señal se parece a sí misma desplazada en el tiempo. En música, eso se traduce en:\n",
    "\n",
    "* Picos periódicos en la autocorrelación = ritmo repetitivo o beats.\n",
    "\n",
    "* Puede ayudarte a detectar tempo, pulsos o patrones repetitivos como los que tienen bases de batería, loops, etc.\n",
    "\n",
    "Por esto sabemos que la autocorrelación es una herramienta realmente potente para analizar el contenido rítmico de un audio. Lo primero que vamos a hacer es definir una función que nos ayude a calcular la autocorrelación de una señal de audio. Para ello nos ayudaremos de la función `correlate` del módulo `scipy.signal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab720dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelacion(x_frame):\n",
    "    \"\"\" Calcula la autocorrelación normalizada de una ventana de señal de audio.\n",
    "    Entrada:\n",
    "        x_frame (numpy.ndarray): Fragmento de señal de audio (ventana temporal)\n",
    "    Salida:\n",
    "        corr_norm (numpy.ndarray): Autocorrelación normalizada desde el retardo cero hacia adelante\n",
    "    \"\"\"\n",
    "    x_frame = x_frame - np.mean(x_frame)\n",
    "    corr = correlate(x_frame, x_frame, mode='full')\n",
    "    mid = len(corr) // 2\n",
    "    return corr[mid:] / np.max(np.abs(corr) + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936bc2f",
   "metadata": {},
   "source": [
    "Una vez hecho esto podemos crear una función `detectar_ritmo` que estime el ritmo de un fragmento de audio usando la correlación, y con este ritmo podemos crear un **círculo en el centro del frame que lata con intensidad variante según el ritmo**. Para ello utilizaremos `sin(2π * t / periodo)`. Con `detectar_ritmo` tenemos todo lo necesario para crear la función `dibujar_circulo_ritmico`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_ritmo(x_frame, fs, fmin=1.5, fmax=8):\n",
    "    \"\"\" Estima el periodo rítmico de un fragmento de audio mediante autocorrelación.\n",
    "    Entrada:\n",
    "        x_frame (numpy.ndarray): Fragmento de señal de audio (ventana temporal)\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "        fmin (float): Frecuencia mínima esperada del ritmo (en Hz)\n",
    "        fmax (float): Frecuencia máxima esperada del ritmo (en Hz)\n",
    "    Salida:\n",
    "        periodo_seg (float): Periodo estimado del ritmo en segundos\n",
    "        corr (numpy.ndarray): Autocorrelación normalizada del fragmento de audio\n",
    "    \"\"\"\n",
    "    corr = autocorrelacion(x_frame)\n",
    "    min_lag = int(fs / fmax)\n",
    "    max_lag = int(fs / fmin)\n",
    "    if max_lag >= len(corr): max_lag = len(corr) - 1\n",
    "    if min_lag >= max_lag: return 0.5, corr  # Valor por defecto\n",
    "    pico = np.argmax(corr[min_lag:max_lag]) + min_lag\n",
    "    periodo_seg = pico / fs\n",
    "    return periodo_seg, corr\n",
    "\n",
    "def dibujar_circulo_ritmico(t_actual, periodo):\n",
    "    \"\"\" Dibuja un círculo que pulsa rítmicamente en el centro del frame según un periodo dado.\n",
    "    Entrada:\n",
    "        t_actual (float): Tiempo actual del video en segundos\n",
    "        periodo (float): Periodo rítmico estimado en segundos\n",
    "    Salida:\n",
    "        None: El círculo se dibuja directamente sobre la figura\n",
    "    \"\"\"\n",
    "    ritmo_osc = 0.5 * (1 + np.sin(2 * np.pi * t_actual / periodo))\n",
    "    color = (ritmo_osc, 0.2, 1 - ritmo_osc)\n",
    "    size = 300 * ritmo_osc + 20\n",
    "    plt.scatter(0.5, 0.5, s=size, c=[color], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b6602",
   "metadata": {},
   "source": [
    "Ahora añadiremos toda esta información al bucle de generación del fotograma. Aprovecharemos la división a trozos de la señal que hemos implementado antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # ------NUEVO------\n",
    "        \n",
    "        # Detección rítmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "        \n",
    "         # ------NUEVO------\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"pruebas/pruebaRitmo.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19952894",
   "metadata": {},
   "source": [
    "Ahora visualizaremos el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cdf852",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"pruebas/pruebaRitmo.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e09a91",
   "metadata": {},
   "source": [
    "Podemos observar como el circulo late de forma constante al ritmo constante delimitado por los beats de fondo del audio original. Sin embargo, este diseño que acabamos de crear tiene **una debilidad importante**, y es que no es capaz de distinguir qué ritmo del audio tiene contenido armónico relevante para nosotros. Debido a que estamos aplicando la autocorrelación directamente sobre la onda real podemos estar obteniendo partes del ritmo que son poco relevantes en armónicamente en la canción.\n",
    "\n",
    "Por ello, para hacer un aislamiento de estas partes poco relevantes y quedarnos con lo que nos interesa de verdad (y en consecuencia hacer que el circulo lata todavía más acorde con la canción), **vamos a aplicar la autocorrelación sobre la envolvente**. De esta forma vamos a obtener los **golpes rítmicos reales** de la señal. Por cada golpe rítmico vamos a generar un flash en el centro del fotograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_frames(x, fs):\n",
    "    \"\"\" Detecta los beats del audio a partir de la envolvente y devuelve sus ubicaciones en frames.\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Señal de audio completa\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "    Salida:\n",
    "        beat_frames (numpy.ndarray): Índices de frame donde se detectan beats en la señal\n",
    "    \"\"\"\n",
    "    # Autocorrelación sobre la envolvente\n",
    "    env_smooth = envolvente(x, fs=fs, tr=0.1)  # más estable\n",
    "    corr_env = autocorrelacion(env_smooth)\n",
    "\n",
    "    # Estimar el tempo global\n",
    "    min_lag = int(fs / 5)    # máx 5 Hz = 300 BPM\n",
    "    max_lag = int(fs / 1.5)  # mín 1.5 Hz = 90 BPM\n",
    "    lag_beat = np.argmax(corr_env[min_lag:max_lag]) + min_lag\n",
    "    periodo_muestras = lag_beat\n",
    "\n",
    "    # Encontrar los picos en la envolvente\n",
    "    peaks, _ = find_peaks(env_smooth, distance=periodo_muestras * 0.8)\n",
    "\n",
    "    # Convertir los picos (en muestras) a tiempos (en segundos) y luego a frames\n",
    "    beat_times = peaks / fs\n",
    "    beat_frames = (beat_times * FPS).astype(int)\n",
    "    return beat_frames\n",
    "\n",
    "def es_beat(i, beat_frames, tolerancia=2):\n",
    "    \"\"\" Determina si un frame está dentro de los limites de un beat detectado.\n",
    "    Entrada:\n",
    "        frame_index (int): Índice del frame actual en el video\n",
    "        beat_frames (list of int): Lista de frames donde se detectaron beats\n",
    "        tolerancia (int): Número de frames de margen alrededor de cada beat\n",
    "    Salida:\n",
    "        es_beat (bool): True si el frame está cerca de un beat, False en caso contrario\n",
    "    \"\"\"\n",
    "    return any(abs(i - bf) <= tolerancia for bf in beat_frames)\n",
    "\n",
    "\n",
    "def dibujar_flash(i, beats):\n",
    "    \"\"\" Dibuja un flash visual en el centro del frame, usado para resaltar beats detectados.\n",
    "    Entrada:\n",
    "        beats (numpy.ndarray): Array con los índices de los fotogramas donde hay un beat\n",
    "    Salida:\n",
    "        None: El flash se dibuja directamente sobre la figura\n",
    "    \"\"\"\n",
    "    # Halo pulsante animado en el centro tras beat\n",
    "    for bf in beats:\n",
    "        frames_from_beat = i - bf\n",
    "        if 0 <= frames_from_beat <= 4:  # duración 4 frames\n",
    "            grow = 1 - frames_from_beat / 4\n",
    "            size = 2000 * grow\n",
    "            alpha = 0.8 * grow\n",
    "            plt.scatter(0.5, 0.5, s=size, c='magenta', alpha=alpha, edgecolors='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d034d",
   "metadata": {},
   "source": [
    "Ahora vamos a ampliar la función `generar_frames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash más visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #------NUEVO------\n",
    "            \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # Detección rítmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"pruebas/pruebaRitmo2.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec458ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Video(\"pruebas/pruebaRitmo2.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471210a",
   "metadata": {},
   "source": [
    "Como vemos, cada vez tarda más en generar los fotogramos puesto que estamso todo el rato añadiendo nueva a información a la funciçon `generar_frames`. Con este último componente en el vídeo ya podemos dar por finalizado todo el proceso de este conversor de audio a vídeo. Sin embargo, nos gustaría añadir un último apartado con algo que nos parece curioso pero que no queremos incluir en la versión final ya que el resultado es bastante epileptico y la idea está sacada de Internet. Lo dejamos como celda opcional a ejecutar. Si no quieres hacerlo, salta directamente al apartado del resultado final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c16bca",
   "metadata": {},
   "source": [
    "### Opcional: variación de color de fondo con centroide espectral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade7257",
   "metadata": {},
   "source": [
    "Para darle un toque final al programa estábamos pensando en jugar con la tonalidad del audio. Queriamos encontrar algo que variase según los cambios de tonalidad (cambio de grave a agudo etc). Se nos ocurrió que podríamos variar el color de fondo y, buscando por Internet, encontramos algo que podemos obtener a partir del espectro de la señal: **el centroide espectral**. El centroide espectral indica la \"brillantez\" del sonido, es decir, qué tan concentrada está la energía en frecuencias altas.\n",
    "\n",
    "* Un centroide alto → sonidos agudos o brillantes → fondo más claro o cálido.\n",
    "\n",
    "* Un centroide bajo → sonidos graves o apagados → fondo más oscuro o frío."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "from vozyaudio import espectro\n",
    "import numpy as np\n",
    "\n",
    "import colorsys\n",
    "from vozyaudio import espectro\n",
    "import numpy as np\n",
    "\n",
    "def color_fondo_por_centroide(X, fa, x_frame, fs):\n",
    "    \"\"\" Genera un color RGB para el fondo en función del centroide espectral del frame.\n",
    "    Entrada:\n",
    "        X (np.ndarray): Módulo del espectro del frame de audio (magnitudes)\n",
    "        fa (np.ndarray): Vector de frecuencias correspondientes al espectro\n",
    "        x_frame (np.ndarray): Fragmento de señal de audio correspondiente al frame\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "    Salida:\n",
    "        fondo_color (tuple): Color RGB normalizado (0-1) para usar como fondo del frame\n",
    "    \"\"\"\n",
    "    # Evitar errores si X está vacío\n",
    "    if np.sum(X) == 0 or len(fa) != len(X):\n",
    "        return (0, 0, 0.1)  # fondo oscuro por defecto\n",
    "\n",
    "    # Centroide espectral\n",
    "    centroide = np.sum(fa * X) / (np.sum(X) + 1e-9)\n",
    "\n",
    "    # Normalizar centroide al rango 0–1 basado en un rango realista (0–4000 Hz)\n",
    "    centroide_norm = np.clip(centroide / 4000, 0, 1)\n",
    "\n",
    "    # Usar centroide como matiz, pero también afectar brillo\n",
    "    h = centroide_norm                   # matiz (rojo ↔ azul)\n",
    "    s = 0.9                              # saturación constante\n",
    "    v = 0.3 + 0.7 * centroide_norm       # más agudo → más brillante\n",
    "\n",
    "    fondo_color = colorsys.hsv_to_rgb(h, s, v)\n",
    "    return fondo_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79de99",
   "metadata": {},
   "source": [
    "Con esta función, la función `generar_frames` quedaría así."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85257f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash más visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        fondo_color =  color_fondo_por_centroide(X,fa,x_frame,fs)\n",
    "        fig.set_facecolor(fondo_color)\n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        # Detección rítmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"pruebas/pruebaColor.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"pruebas/pruebaColor.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3be13d",
   "metadata": {},
   "source": [
    "### Resultado final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bbe5d",
   "metadata": {},
   "source": [
    "Para finalizar, vamos a dejar una celda con la función final de `generar_frames` y las rutas para poder cambiar fácilmente los audios y poder testear audios distintos al de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "AUDIO_PATH = 'audios/music.wav' # Ruta del audio que vamos a usar\n",
    "FPS = 25 # Número de fotogramas por segundo del vídeo resultante\n",
    "FRAME_FOLDER = 'fotogramas' # Ruta de la carpeta donde iremos guardando los fotogramas\n",
    "N_BARRAS = 60  # Número de barras del espectro\n",
    "\n",
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash más visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # Detección rítmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"final.mp4\"\n",
    "crear_video(AUDIO_PATH,out)\n",
    "Video(\"final.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99b5ac",
   "metadata": {},
   "source": [
    "### Resumen de PixelSounds: visualización de audio en video con Python\n",
    "\n",
    "#### Objetivo general\n",
    "\n",
    "Nuestro objetivo principal es el de crear un video dinámico a partir de un archivo de audio, donde la visualización reaccione a distintos descriptores del sonido como ritmo, espectro, envolvente, etc., utilizando exclusivamente Python.\n",
    "\n",
    "#### Resumen de lo aprendido y conceptos explicados\n",
    "\n",
    "##### Descriptores de audio\n",
    "Hemos sabido comprender y aplicar diferentes descriptores que capturan distintas propiedades del sonido:\n",
    "\n",
    "1. Envolvente de amplitud\n",
    "\n",
    "* Representa la variación de la energía de la señal a lo largo del tiempo.\n",
    "\n",
    "* Útil para detectar intensidad, ataques o silencios.\n",
    "\n",
    "2. Espectro de frecuencias\n",
    "\n",
    "* Se obtiene mediante la Transformada de Fourier.\n",
    "\n",
    "* Permite ver qué frecuencias están presentes en un instante.\n",
    "\n",
    "3. Centroide espectral\n",
    "\n",
    "* Indica el “centro de gravedad” del espectro.\n",
    "\n",
    "* Cuanto más alto, más brillante o agudo se percibe el sonido.\n",
    "\n",
    "4. Autocorrelación\n",
    "\n",
    "* Herramienta para detectar periodicidad o repetición.\n",
    "\n",
    "* Aplicada sobre la envolvente para estimar el ritmo o tempo.\n",
    "\n",
    "5. Detección de beats\n",
    "\n",
    "* Se basa en picos periódicos de energía detectados en la envolvente.\n",
    "\n",
    "* Permite sincronizar efectos visuales con los golpes musicales.\n",
    "\n",
    "##### Procesamiento por fotogramas\n",
    "* El audio se divide en ventanas temporales por fotogramas de vídeo.\n",
    "\n",
    "* Cada fragmento se analiza individualmente para obtener descriptores y generar visuales sincronizados.\n",
    "\n",
    "##### Relación audio → imagen\n",
    "Hemos aprendido a convertir propiedades del audio en parámetros visuales:\n",
    "\n",
    "* Pitch o frecuencia → posición vertical.\n",
    "\n",
    "* Envolvente → tamaño o brillo.\n",
    "\n",
    "* Centroide espectral → color.\n",
    "\n",
    "* Beat → efectos puntuales (flashes, cambios bruscos).\n",
    "\n",
    "Hemos aplicado conceptos de visualización dinámica, donde el vídeo no es estático, sino que evoluciona en función del sonido.\n",
    "\n",
    "##### Programación y diseño de sistema\n",
    "* Hemos optado por un diseño modular del sistema: funciones pequeñas y reutilizables para análisis, extracción de descriptores y visualización.\n",
    "\n",
    "* Hemos hecho uso de librerías como matplotlib, scipy, numpy, colorsys.\n",
    "\n",
    "* Hemos realizado la automatización de video + audio con ffmpeg.\n",
    "\n",
    "#### Funciones creadas\n",
    "\n",
    "* `normalizar(v)`: normaliza valores en rango [0, 1].\n",
    "\n",
    "* `autocorrelacion(x_frame)`: autocorrelación normalizada de un fragmento.\n",
    "\n",
    "* `detectar_ritmo(x_frame, fs, ...)`: calcula el periodo dominante.\n",
    "\n",
    "* `es_beat(frame_index, beat_frames, ...)`: determina si un frame es un beat.\n",
    "\n",
    "* `dibujar_flash()`: dibuja un flash central.\n",
    "\n",
    "* `dibujar_barras(ax, X_resampled, N_BARRAS)`: visualización espectral.\n",
    "\n",
    "* `color_fondo_por_centroide(x_frame, fs)`: color de fondo basado en centroide.\n",
    "\n",
    "* `beat_frames(x, fs)`: calcula los beats a partir de la envolvente.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
