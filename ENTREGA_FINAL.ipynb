{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00973eb6",
   "metadata": {},
   "source": [
    "# PixelSounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f311bf",
   "metadata": {},
   "source": [
    "⚠️ **Advertencia de Fotosensibilidad**  \n",
    "> Este notebook incluye secuencias de vídeo generadas a partir de señales de audio, que pueden contener parpadeos rápidos, patrones repetitivos o transiciones de alto contraste.  \n",
    "> \n",
    "> Las personas con fotosensibilidad o epilepsia fotosensible deben evitar la reproducción de estos vídeos, o asegurarse de visualizarlos con las precauciones adecuadas (pantalla reducida, brillo bajo, sin visión directa).\n",
    "\n",
    "IMPORTANTE:\n",
    "* Recomendamos ejecutar este cuaderno desde el principio y en orden puesto que hay variables que se sobreescriben.\n",
    "* Recomendamos ejecutar este cuaderno en el **entorno nativo de Jupyter Notebook** (al que se accede mediante Anaconda Powershell Prompt) o en **Visual Studio Code**, puesto que son los únicos en los que hemos testeado el correcto funcionamiento. Si se ejecuta en otros entornos, **no podemos asegurar el correcto funcionamiento de todos los componentes del cuaderno**, pusto que, por ejemplo, cada entorno soporta distintos formatos de vídeo o audio.\n",
    "* Para el correcto funcionamiento del programa **HAY QUE TENER CORRECTAMENTE INSTALADO FFMPEG**, con su carpeta en el disco local y añadida al PATH del sistema. Esto se debe a que la libreria `ffmpeg-python` hace uso de ffmpeg. \n",
    "Para más información, visitar https://www.wikihow.com/Install-FFmpeg-on-Windows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a66fb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Lista de módulos requeridos: {import_name: pip_package_name} ===\n",
    "modules = {\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"matplotlib\": \"matplotlib\",\n",
    "    \"IPython\": \"ipywidgets\",\n",
    "    \"scipy\": \"scipy\",\n",
    "    \"librosa\": \"librosa\",\n",
    "    \"ipywidgets\": \"ipywidgets\",\n",
    "    \"bqplot\": \"bqplot\",\n",
    "    \"pyaudio\": \"pyaudio\",\n",
    "    \"ffmpeg\": \"ffmpeg-python\",\n",
    "    \"cv2\": \"opencv-python\",\n",
    "    \"imageio\": \"imageio\"\n",
    "    # \"os\", \"shutil\", \"subprocess\", \"threading\", \"colorsys\" → módulos estándar, no necesitan instalación\n",
    "}\n",
    "\n",
    "# === Instalar si faltan ===\n",
    "for mod, pip_name in modules.items():\n",
    "    try:\n",
    "        __import__(mod.split(\".\")[0])  # para casos como imageio.v2\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {pip_name}...\")\n",
    "        %pip install {pip_name}\n",
    "\n",
    "# === Built-in ===\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import threading\n",
    "import colorsys\n",
    "\n",
    "# === NumPy y SciPy ===\n",
    "import numpy as np\n",
    "from scipy import signal, ndimage, interpolate\n",
    "from scipy.signal import (\n",
    "    correlate, freqz, firwin, iirfilter, get_window,\n",
    "    resample, lfilter, find_peaks\n",
    ")\n",
    "from scipy.io import wavfile\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.fftpack import dct, idct, dctn, idctn\n",
    "from scipy.linalg import solve_toeplitz\n",
    "\n",
    "# === Audio ===\n",
    "import pyaudio\n",
    "import librosa\n",
    "import librosa.display\n",
    "from librosa import piptrack\n",
    "import ffmpeg\n",
    "\n",
    "# === Imágenes y vídeo ===\n",
    "import cv2\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "# === Visualización en notebooks ===\n",
    "import matplotlib.pyplot as plt\n",
    "import bqplot as bq\n",
    "from IPython.display import Audio, Video, display\n",
    "import IPython.display as ipd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# === Módulo Voz ===\n",
    "from vozyaudio import lee_audio, sonido, envolvente, track_pitch, espectro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede63ce",
   "metadata": {},
   "source": [
    "# Conversión Audio a Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf4fc5",
   "metadata": {},
   "source": [
    "La conversión de audio a video es una técnica que permite visualizar las propiedades temporales y espectrales de una señal sonora, convirtiendo sus características en representaciones visuales que pueden ser entendidas o incluso reconstruidas por humanos o máquinas. Esta categoría engloba dos enfoques complementarios desarrollados en este proyecto.\n",
    "\n",
    "---\n",
    "\n",
    "__*PixelSounds: Visualización de parámetros de audio en vídeo*__\n",
    "\n",
    "Este bloque se centra en la representación visual de características acústicas de una señal sonora en tiempo real o desde archivos de audio.\n",
    "\n",
    "- Se han implementado diferentes sintetizadores (aditivo, FM, sustractivo) para la generación de señales.\n",
    "- Mediante herramientas de visualización, se muestran:\n",
    "  - La forma de onda y su evolución en el tiempo.\n",
    "  - El espectro de la señal y su envolvente.\n",
    "  - Parámetros derivados como el pitch, la frecuencia de resonancia o la respuesta en frecuencia de filtros.\n",
    "\n",
    "Estas representaciones ayudan a comprender visualmente cómo se comportan los parámetros acústicos en distintas condiciones de síntesis o procesado de la señal.\n",
    "\n",
    "---\n",
    "\n",
    "__*PixelSounds: Codec Reversible de Audio y Vídeo*__\n",
    "\n",
    "Esta segunda aproximación propone un sistema *reversible* que convierte un archivo de audio en una secuencia de imágenes tipo vídeo —y viceversa—, codificando las propiedades del sonido en cada frame.\n",
    "\n",
    "- El codificador genera imágenes desde bloques enventanados del audio.\n",
    "- El primer frame incluye los metadatos necesarios para decodificar.\n",
    "- Los siguientes frames codifican los bloques de señal mediante distintas estrategias:\n",
    "  - Amplitud normalizada.\n",
    "  - FFT (módulo y fase).\n",
    "  - Energía en bandas filtradas (baja, media, alta).\n",
    "- El decodificador reconstruye el audio original mediante *overlap-add*, usando la información contenida en cada imagen.\n",
    "\n",
    "Este enfoque puede servir como base para transmisiones visuales de audio, compresión alternativa o experimentación creativa audiovisual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd81630",
   "metadata": {},
   "source": [
    "## PixelSounds: Visualización de parámetros de audio en vídeo\n",
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2c80c",
   "metadata": {},
   "source": [
    "El primer conversor que vamos a desarrollar en el proyecto PixelSounds es un conversor de audio a vídeo. Sin embargo, no va a ser cualquier tipo de vídeo. Vamos a tratar de generar un vídeo que nos permita visualizar en una primera vista algunas características del audio que hay detrás. Para ello vamos a extraer algunos descriptores del audio (los cuales especificaremos más adelante) y vamos a transformarlos en algo visual. De esta forma, iremos generando fotogramas y mediante la librería `ffmpeg` los uniremos para crear un video. En esta primera parte del cuaderno vamos a ir desgranando una por una las transformaciones que haremos para visualizar los distintos descriptores. Después uniremos todas las funciones para generar un fotograma. Tras aprender a generar un fotograma, podremos generar muchos para crear el vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916d606",
   "metadata": {},
   "source": [
    "### Los primeros pasos\n",
    "Antes de comenzar a hacer nada, vamos a definir algunos parámetros básicos que vamos a utilizar más adelante. La configuración del codificador consta de los siguientes parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44b2855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando carpeta existente: fotogramas/\n",
      "Eliminando carpeta existente: exports/\n",
      "Carpeta fotogramas creada.\n",
      "Carpeta exports creada.\n"
     ]
    }
   ],
   "source": [
    "# Configuración\n",
    "AUDIO_PATH = 'audios/music.wav' # Ruta del audio que vamos a usar\n",
    "FPS = 25 # Número de fotogramas por segundo del vídeo resultante\n",
    "FRAME_FOLDER = 'fotogramas' # Ruta de la carpeta donde iremos guardando los fotogramas\n",
    "N_BARRAS = 60  # Número de barras del espectro\n",
    "\n",
    "# Borrar las carpetas de salida si existen\n",
    "for d in (\"fotogramas\", \"exports\"):\n",
    "    if os.path.exists(d):\n",
    "        print(f\"Eliminando carpeta existente: {d}/\")\n",
    "        shutil.rmtree(d)\n",
    "    else:\n",
    "        print(f\"No existe: {d}/ — nada que borrar.\")\n",
    "\n",
    "# Crear carpetas de salida\n",
    "frames_dir = f\"fotogramas\"\n",
    "export_dir = f\"exports\"\n",
    "\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "print(f\"Carpeta {frames_dir} creada.\")\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "print(f\"Carpeta {export_dir} creada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252dc1aa",
   "metadata": {},
   "source": [
    "Después de esto, cargaremos el audio y extraeremos su duración, número de muestras, y muestras por fotograma. Además, obtendremos el número de frames que ha de tener el vídeo resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca6dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar audio\n",
    "fs, x = lee_audio(AUDIO_PATH) # Lee el audio\n",
    "x = x.astype(np.float32) # Transforma el tipo de dato\n",
    "dur = len(x) / fs # Duración del audio\n",
    "n_frames = int(FPS * dur) # Número de frames del vídeo\n",
    "samples_per_frame = int(fs / FPS) # Número de muestras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095cf2d0",
   "metadata": {},
   "source": [
    "Tras hacer todo esto ya podemos empezar a generar las primeras componentes del fotograma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b61814",
   "metadata": {},
   "source": [
    "### Círculo que se mueve con el pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c83bf",
   "metadata": {},
   "source": [
    "Lo primero que vamos a agregar al fotograma es un **circulo que se mueve de arriba a abajo con el pitch y cambia de tamaño con la envolvente**. Para este paso, usaremos las funciones `envolvente` y `track_pitch` del módulo `vozyaudio` para extraer la envolvente y estimar el pitch del audio. Después normalizaremos ambos descriptores y los redimensionaremos para que se ajusten al número de frames finales del video. Todo esto lo haremos dentro de la función `obtener_descriptores` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd78f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(v):\n",
    "    \"\"\" Normaliza un vector al rango [0, 1].\n",
    "    Entrada:\n",
    "        v (numpy.ndarray): Vector de valores (por ejemplo, envolvente, pitch, espectro, etc.)\n",
    "    Salida:\n",
    "        v_norm (numpy.ndarray): Vector normalizado en el rango [0, 1]\n",
    "    \"\"\"\n",
    "    return (v - np.min(v)) / (np.max(v) - np.min(v) + 1e-9)\n",
    "\n",
    "def obtener_descriptores(x,fs):\n",
    "    \"\"\" Extrae distintos descriptores de la señal de audio.\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "    Salida:\n",
    "        pitch_frame (numpy.ndarray): Vector con los valores de la estimación del pitch normalizados y redimensionados al número de frames\n",
    "        env_frame (numpy.ndarray): Vector con los valores de la envolvente normalizados y redimensionados al número de frames \n",
    "    \"\"\"\n",
    "    # Extraemos los descriptores de envolvente y estimación de pitch\n",
    "    env = envolvente(x, fs=fs) # Extraer envolvente\n",
    "    pitch = track_pitch(x, fs) # Estimar pitch\n",
    "    pitch = np.nan_to_num(pitch)  # Reemplaza NaNs por 0\n",
    "    \n",
    "    env = normalizar(env) # Normalizar ambos arrays\n",
    "    pitch = normalizar(pitch)\n",
    "    \n",
    "    # Redimensionar descriptores al número de frames\n",
    "    env_frame = np.interp(np.linspace(0, len(env), n_frames), np.arange(len(env)), env)\n",
    "    pitch_frame = np.interp(np.linspace(0, len(pitch), n_frames), np.arange(len(pitch)), pitch)\n",
    "\n",
    "    return pitch_frame, env_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527b556",
   "metadata": {},
   "source": [
    "Con estas variables ya tenemos todo lo necesario para generar el primer componente que variará durante el vídeo. Lo siguiente que debemos hacer es definir una función que dibuje el circulo en pantalla variando según estos valores. A esta función la llamaremos `dibujar_particula`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a23679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_particula(pitch, env):\n",
    "    \"\"\" Dibuja una partícula que se mueve según el pitch y cambia de tamaño según la envolvente del audio\n",
    "    Entrada:\n",
    "        pitch (float) : Valor de estimación del pitch en un fotograma determinado\n",
    "        env (float) : Valor de la envolvente del audio en un fotograma determinado\n",
    "    Salida:\n",
    "        None: La función solo dibuja en la figura y no devuelve nada\n",
    "    \"\"\"\n",
    "    y_pos = pitch\n",
    "    size = 100 + env * 300\n",
    "    color = (1.0, env, pitch)\n",
    "    plt.scatter(0.5, y_pos, s=size, c=[color], alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e0694",
   "metadata": {},
   "source": [
    "Para visualizar que el resultado es el que esperamos tenemos que definir una función básica de generación de frames. Esta función la iremos ampliando a medida que vayamos añadiendo más componentes al fotograma. Cuando tengamos todos los fotogramas generados los uniremos mediante la función `crear_video`  para ver el resultado. Esta función hace uso de `subprocess` y de `ffmpeg` para crear el video de manera rápida simplemente pasándole el archivo `generarVideo.bat` que se encargará de ejecutar los comandos ffmpeg. La usaremos mucho a lo largo del cuaderno. De momento, probemos a generar los fotogramas con solo esta componente y ver el video resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f67373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig,ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "out = \"exports/pruebaPitch.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8810ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_video(audio_path, out_path, frames_dir=\"fotogramas\", framerate=25):\n",
    "    try:\n",
    "        if os.path.exists(out_path):\n",
    "            os.remove(out_path)\n",
    "\n",
    "        input_video = os.path.join(frames_dir, \"frame_%04d.png\")\n",
    "\n",
    "        # Crear entradas por separado\n",
    "        video_input = ffmpeg.input(input_video, framerate=framerate)\n",
    "        audio_input = ffmpeg.input(audio_path)\n",
    "\n",
    "        (\n",
    "            ffmpeg\n",
    "            .output(\n",
    "                video_input, audio_input,\n",
    "                out_path,\n",
    "                vcodec='libx264',\n",
    "                acodec=\"pcm_s32le\", # Formato de audio soportado por VSCode\n",
    "                pix_fmt='yuv420p',\n",
    "                shortest=None\n",
    "            )\n",
    "            .run(overwrite_output=True)\n",
    "        )\n",
    "\n",
    "        print(f\"\\n[OK] Video exportado como: {out_path}\")\n",
    "    except ffmpeg.Error as e:\n",
    "        print(\"Error al crear el video:\")\n",
    "        print(e.stderr.decode() if hasattr(e, \"stderr\") else e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc26ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video exportado como: exports/pruebaPitch.mp4\n"
     ]
    }
   ],
   "source": [
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bfa215",
   "metadata": {},
   "source": [
    "A continuación visualizaremos el resultado en la siguiente celda. Podemos ver como la posición vertical varia con el pitch y la intensidad del halo varia con la envolvente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a4feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"exports/pruebaPitch.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b5d84",
   "metadata": {},
   "source": [
    "### Barras espectrales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401dd239",
   "metadata": {},
   "source": [
    "Después de hacer que la estimación del pitch sea visible en el video vamos a añadir algún componente que nos muestre de alguna forma la **energía de cada banda de frecuencia en la señal**. Para ello vamos a usar barras espectrales que aumenten y disminuyan en función de la energía espectral de la señal. Para ello haremos uso de la función `espectro`. Lo que vamos a hacer es dividir la señal en trozos. De cada trozo extraeremos su información espectral y las adapataremos al componente visual de la barras. Todo esto lo introduciremos dentro de la función `generar_frames`.\n",
    "\n",
    "La primera función que vamos a desarrollar en este bloque es la de `dibujar_barras`. Se encargará de plotear las barras en los fotogramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d660a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_barras(X_resampled, N_BARRAS):\n",
    "    \"\"\" Dibuja barras verticales que representan la energía en diferentes bandas de frecuencia.\n",
    "    Entrada:\n",
    "        X_resampled (numpy.ndarray): Vector con las amplitudes espectrales reescaladas a N_BARRAS bandas\n",
    "        N_BARRAS (int): Número total de barras a dibujar\n",
    "    Salida:\n",
    "        None: Las barras se dibujan directamente sobre la figura\n",
    "    \"\"\"\n",
    "    bar_width = 1 / N_BARRAS\n",
    "    for j in range(N_BARRAS):\n",
    "        height = X_resampled[j]\n",
    "        color = (0.1, 0.8 * height, 1.0)\n",
    "        plt.bar(j * bar_width, height, width=bar_width*0.8, color=color, align='edge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09cdbc",
   "metadata": {},
   "source": [
    "Tras esto, vamos a añadir a `generar_frames` la parte de dividir la señal en trozos, extraer su espectro y dibujar las barras espectrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee1ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %[OK] Video exportado como: exports/pruebaBarras.mp4\n"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        # ------NUEVO--------\n",
    "        \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # ------NUEVO--------\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"exports/pruebaBarras.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649675a",
   "metadata": {},
   "source": [
    "En la siguiente celda podemos observar como las barras verticales varian con la energía de la señal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a44c92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"exports/pruebaBarras.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ddf5c",
   "metadata": {},
   "source": [
    "### Autocorrelación y patrones rítmicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8dc030",
   "metadata": {},
   "source": [
    "La autocorrelación mide cómo una señal se parece a sí misma desplazada en el tiempo. En música, eso se traduce en:\n",
    "\n",
    "* Picos periódicos en la autocorrelación = ritmo repetitivo o beats.\n",
    "\n",
    "* Puede ayudarte a detectar tempo, pulsos o patrones repetitivos como los que tienen bases de batería, loops, etc.\n",
    "\n",
    "Por esto sabemos que la autocorrelación es una herramienta realmente potente para analizar el contenido rítmico de un audio. Lo primero que vamos a hacer es definir una función que nos ayude a calcular la autocorrelación de una señal de audio. Para ello nos ayudaremos de la función `correlate` del módulo `scipy.signal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c09f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelacion(x_frame):\n",
    "    \"\"\" Calcula la autocorrelación normalizada de una ventana de señal de audio.\n",
    "    Entrada:\n",
    "        x_frame (numpy.ndarray): Fragmento de señal de audio (ventana temporal)\n",
    "    Salida:\n",
    "        corr_norm (numpy.ndarray): Autocorrelación normalizada desde el retardo cero hacia adelante\n",
    "    \"\"\"\n",
    "    x_frame = x_frame - np.mean(x_frame)\n",
    "    corr = correlate(x_frame, x_frame, mode='full')\n",
    "    mid = len(corr) // 2\n",
    "    return corr[mid:] / np.max(np.abs(corr) + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8524c7e",
   "metadata": {},
   "source": [
    "Una vez hecho esto podemos crear una función `detectar_ritmo` que estime el ritmo de un fragmento de audio usando la correlación, y con este ritmo podemos crear un **círculo en el centro del frame que lata con intensidad variante según el ritmo**. Para ello utilizaremos `sin(2π * t / periodo)`. Con `detectar_ritmo` tenemos todo lo necesario para crear la función `dibujar_circulo_ritmico`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2ef1d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_ritmo(x_frame, fs, fmin=1.5, fmax=8):\n",
    "    \"\"\" Estima el periodo rítmico de un fragmento de audio mediante autocorrelación.\n",
    "    Entrada:\n",
    "        x_frame (numpy.ndarray): Fragmento de señal de audio (ventana temporal)\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "        fmin (float): Frecuencia mínima esperada del ritmo (en Hz)\n",
    "        fmax (float): Frecuencia máxima esperada del ritmo (en Hz)\n",
    "    Salida:\n",
    "        periodo_seg (float): Periodo estimado del ritmo en segundos\n",
    "        corr (numpy.ndarray): Autocorrelación normalizada del fragmento de audio\n",
    "    \"\"\"\n",
    "    corr = autocorrelacion(x_frame)\n",
    "    min_lag = int(fs / fmax)\n",
    "    max_lag = int(fs / fmin)\n",
    "    if max_lag >= len(corr): max_lag = len(corr) - 1\n",
    "    if min_lag >= max_lag: return 0.5, corr  # Valor por defecto\n",
    "    pico = np.argmax(corr[min_lag:max_lag]) + min_lag\n",
    "    periodo_seg = pico / fs\n",
    "    return periodo_seg, corr\n",
    "\n",
    "def dibujar_circulo_ritmico(t_actual, periodo):\n",
    "    \"\"\" Dibuja un círculo que pulsa rítmicamente en el centro del frame según un periodo dado.\n",
    "    Entrada:\n",
    "        t_actual (float): Tiempo actual del video en segundos\n",
    "        periodo (float): Periodo rítmico estimado en segundos\n",
    "    Salida:\n",
    "        None: El círculo se dibuja directamente sobre la figura\n",
    "    \"\"\"\n",
    "    ritmo_osc = 0.5 * (1 + np.sin(2 * np.pi * t_actual / periodo))\n",
    "    color = (ritmo_osc, 0.2, 1 - ritmo_osc)\n",
    "    size = 300 * ritmo_osc + 20\n",
    "    plt.scatter(0.5, 0.5, s=size, c=[color], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bb15d7",
   "metadata": {},
   "source": [
    "Ahora añadiremos toda esta información al bucle de generación del fotograma. Aprovecharemos la división a trozos de la señal que hemos implementado antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d653e3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %[OK] Video exportado como: exports/pruebaRitmo.mp4\n"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # ------NUEVO------\n",
    "        \n",
    "        # Detección rítmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "        \n",
    "         # ------NUEVO------\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"exports/pruebaRitmo.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48338ff5",
   "metadata": {},
   "source": [
    "Ahora visualizaremos el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b9c0c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"exports/pruebaRitmo.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c649394",
   "metadata": {},
   "source": [
    "Podemos observar como el circulo late de forma constante al ritmo constante delimitado por los beats de fondo del audio original. Sin embargo, este diseño que acabamos de crear tiene **una debilidad importante**, y es que no es capaz de distinguir qué ritmo del audio tiene contenido armónico relevante para nosotros. Debido a que estamos aplicando la autocorrelación directamente sobre la onda real podemos estar obteniendo partes del ritmo que son poco relevantes en armónicamente en la canción.\n",
    "\n",
    "Por ello, para hacer un aislamiento de estas partes poco relevantes y quedarnos con lo que nos interesa de verdad (y en consecuencia hacer que el circulo lata todavía más acorde con la canción), **vamos a aplicar la autocorrelación sobre la envolvente**. De esta forma vamos a obtener los **golpes rítmicos reales** de la señal. Por cada golpe rítmico vamos a generar un flash en el centro del fotograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a38f1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_frames(x, fs):\n",
    "    \"\"\" Detecta los beats del audio a partir de la envolvente y devuelve sus ubicaciones en frames.\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Señal de audio completa\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "    Salida:\n",
    "        beat_frames (numpy.ndarray): Índices de frame donde se detectan beats en la señal\n",
    "    \"\"\"\n",
    "    # Autocorrelación sobre la envolvente\n",
    "    env_smooth = envolvente(x, fs=fs, tr=0.1)  # más estable\n",
    "    corr_env = autocorrelacion(env_smooth)\n",
    "\n",
    "    # Estimar el tempo global\n",
    "    min_lag = int(fs / 5)    # máx 5 Hz = 300 BPM\n",
    "    max_lag = int(fs / 1.5)  # mín 1.5 Hz = 90 BPM\n",
    "    lag_beat = np.argmax(corr_env[min_lag:max_lag]) + min_lag\n",
    "    periodo_muestras = lag_beat\n",
    "\n",
    "    # Encontrar los picos en la envolvente\n",
    "    peaks, _ = find_peaks(env_smooth, distance=periodo_muestras * 0.8)\n",
    "\n",
    "    # Convertir los picos (en muestras) a tiempos (en segundos) y luego a frames\n",
    "    beat_times = peaks / fs\n",
    "    beat_frames = (beat_times * FPS).astype(int)\n",
    "    return beat_frames\n",
    "\n",
    "def es_beat(i, beat_frames, tolerancia=2):\n",
    "    \"\"\" Determina si un frame está dentro de los limites de un beat detectado.\n",
    "    Entrada:\n",
    "        frame_index (int): Índice del frame actual en el video\n",
    "        beat_frames (list of int): Lista de frames donde se detectaron beats\n",
    "        tolerancia (int): Número de frames de margen alrededor de cada beat\n",
    "    Salida:\n",
    "        es_beat (bool): True si el frame está cerca de un beat, False en caso contrario\n",
    "    \"\"\"\n",
    "    return any(abs(i - bf) <= tolerancia for bf in beat_frames)\n",
    "\n",
    "\n",
    "def dibujar_flash(i, beats):\n",
    "    \"\"\" Dibuja un flash visual en el centro del frame, usado para resaltar beats detectados.\n",
    "    Entrada:\n",
    "        beats (numpy.ndarray): Array con los índices de los fotogramas donde hay un beat\n",
    "    Salida:\n",
    "        None: El flash se dibuja directamente sobre la figura\n",
    "    \"\"\"\n",
    "    # Halo pulsante animado en el centro tras beat\n",
    "    for bf in beats:\n",
    "        frames_from_beat = i - bf\n",
    "        if 0 <= frames_from_beat <= 4:  # duración 4 frames\n",
    "            grow = 1 - frames_from_beat / 4\n",
    "            size = 2000 * grow\n",
    "            alpha = 0.8 * grow\n",
    "            plt.scatter(0.5, 0.5, s=size, c='magenta', alpha=alpha, edgecolors='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa31e2e",
   "metadata": {},
   "source": [
    "Ahora vamos a ampliar la función `generar_frames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79f7dafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %[OK] Video exportado como: exports/pruebaRitmo2.mp4\n"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash más visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #------NUEVO------\n",
    "            \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # Detección rítmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"exports/pruebaRitmo2.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f340257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"exports/pruebaRitmo2.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc57a59",
   "metadata": {},
   "source": [
    "Como vemos, cada vez tarda más en generar los fotogramos puesto que estamso todo el rato añadiendo nueva a información a la funciçon `generar_frames`. Con este último componente en el vídeo ya podemos dar por finalizado todo el proceso de este conversor de audio a vídeo. Sin embargo, nos gustaría añadir un último apartado con algo que nos parece curioso pero que no queremos incluir en la versión final ya que el resultado es bastante epileptico y la idea está sacada de Internet. Lo dejamos como celda opcional a ejecutar. Si no quieres hacerlo, salta directamente al apartado del resultado final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2768fa9f",
   "metadata": {},
   "source": [
    "### Opcional: variación de color de fondo con centroide espectral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83e9e8",
   "metadata": {},
   "source": [
    "Para darle un toque final al programa estábamos pensando en jugar con la tonalidad del audio. Queriamos encontrar algo que variase según los cambios de tonalidad (cambio de grave a agudo etc). Se nos ocurrió que podríamos variar el color de fondo y en la práctica 8 de la asignatura encontramos algo que podemos obtener a partir del espectro de la señal: **el centroide espectral**. El centroide espectral indica la \"brillantez\" del sonido, es decir, qué tan concentrada está la energía en frecuencias altas.\n",
    "\n",
    "* Un centroide alto → sonidos agudos o brillantes → fondo más claro o cálido.\n",
    "\n",
    "* Un centroide bajo → sonidos graves o apagados → fondo más oscuro o frío."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c96a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_fondo_por_centroide(X, fa):\n",
    "    \"\"\" Genera un color RGB para el fondo en función del centroide espectral del frame.\n",
    "    Entrada:\n",
    "        X (numpy.ndarray): Módulo del espectro del frame de audio (magnitudes)\n",
    "        fa (numpy.ndarray): Vector de frecuencias correspondientes al espectro\n",
    "    Salida:\n",
    "        fondo_color (tuple): Color RGB normalizado (0-1) para usar como fondo del frame\n",
    "    \"\"\"\n",
    "    # Evitar errores si X está vacío\n",
    "    if np.sum(X) == 0 or len(fa) != len(X):\n",
    "        return (0, 0, 0.1)  # fondo oscuro por defecto\n",
    "\n",
    "    # Centroide espectral\n",
    "    centroide = np.sum(fa * X) / (np.sum(X) + 1e-9)\n",
    "\n",
    "    # Normalizar centroide al rango 0–1 basado en un rango realista (0–4000 Hz)\n",
    "    centroide_norm = np.clip(centroide / 4000, 0, 1)\n",
    "\n",
    "    # Usar centroide como matiz, pero también afectar brillo\n",
    "    h = centroide_norm                   # matiz (rojo ↔ azul)\n",
    "    s = 0.9                              # saturación constante\n",
    "    v = 0.3 + 0.7 * centroide_norm       # más agudo → más brillante\n",
    "\n",
    "    fondo_color = colorsys.hsv_to_rgb(h, s, v)\n",
    "    return fondo_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea8c816",
   "metadata": {},
   "source": [
    "Con esta función, la función `generar_frames` quedaría así."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9535e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %\n",
      "[OK] Video exportado como: exports/pruebaColor.mp4\n"
     ]
    }
   ],
   "source": [
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash más visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        fondo_color =  color_fondo_por_centroide(X,fa)\n",
    "        fig.set_facecolor(fondo_color)\n",
    "        \n",
    "        #------NUEVO------\n",
    "        \n",
    "        # Detección rítmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"exports/pruebaColor.mp4\"\n",
    "crear_video(AUDIO_PATH,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f83cf550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"exports/pruebaColor.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdccc4f",
   "metadata": {},
   "source": [
    "### Resultado final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e63de",
   "metadata": {},
   "source": [
    "Para finalizar, vamos a dejar una celda con la función final de `generar_frames` y las rutas para poder cambiar fácilmente los audios y poder testear audios distintos al de ejemplo. Recomendamos testear con audios de duración no superior a 15 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad816a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Completado 100.00 %\n",
      "[OK] Video exportado como: exports/final.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"exports/final.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuración\n",
    "AUDIO_PATH = 'audios/music.wav' # Ruta del audio que vamos a usar\n",
    "FPS = 25 # Número de fotogramas por segundo del vídeo resultante\n",
    "FRAME_FOLDER = 'fotogramas' # Ruta de la carpeta donde iremos guardando los fotogramas\n",
    "N_BARRAS = 60  # Número de barras del espectro\n",
    "\n",
    "# Cargar audio\n",
    "fs, x = lee_audio(AUDIO_PATH) # Lee el audio\n",
    "x = x.astype(np.float32) # Transforma el tipo de dato\n",
    "dur = len(x) / fs # Duración del audio\n",
    "n_frames = int(FPS * dur) # Número de frames del vídeo\n",
    "samples_per_frame = int(fs / FPS) # Número de muestras \n",
    "\n",
    "def generar_frames(x,fs, FRAME_FOLDER):\n",
    "    \"\"\" Genera los frames para el vídeo y los guarda en la carpeta FRAME_FOLDER\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Vector de valores de la señal\n",
    "        fs (int): Frecuencia de muestreo de la señal\n",
    "        FRAME_FOLDER (string): Ruta de la carpeta destino\n",
    "    Salida:\n",
    "        None: No devuelve nada, solo genera los fotogramas      \n",
    "    \"\"\"\n",
    "    pitch_frame, env_frame = obtener_descriptores(x,fs)\n",
    "    \n",
    "    print(\"Generando frames...\")\n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / (n_frames-1)) * 100\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        \n",
    "        beats = beat_frames(x,fs)\n",
    "        \n",
    "        # Flash más visible en beat\n",
    "        if es_beat(i,beats,2):  # mayor tolerancia\n",
    "            plt.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "        \n",
    "        dibujar_flash(i,beats)\n",
    "            \n",
    "        #  Obtener trozo de señal actual\n",
    "        start = i * samples_per_frame\n",
    "        end = min(len(x), start + samples_per_frame)\n",
    "        x_frame = x[start:end]\n",
    "\n",
    "        # Espectro (resample a N barras)\n",
    "        X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = resample(X, N_BARRAS)\n",
    "        X_resampled = normalizar(X_resampled)\n",
    "        \n",
    "        # Detección rítmica simple\n",
    "        periodo, corr = detectar_ritmo(x_frame, fs)\n",
    "        \n",
    "        # Calcula un pulso visual que oscila con el ritmo detectado\n",
    "        t_actual = i / FPS\n",
    "        \n",
    "        dibujar_circulo_ritmico(t_actual,periodo)\n",
    "\n",
    "        # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "        dibujar_particula(pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        dibujar_barras(X_resampled, N_BARRAS)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "generar_frames(x,fs,FRAME_FOLDER)\n",
    "\n",
    "out = \"exports/final.mp4\"\n",
    "crear_video(AUDIO_PATH,out)\n",
    "Video(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e776ad",
   "metadata": {},
   "source": [
    "### Resumen de PixelSounds: visualización de audio en video con Python\n",
    "\n",
    "#### Objetivo general\n",
    "\n",
    "Nuestro objetivo principal es el de crear un video dinámico a partir de un archivo de audio, donde la visualización reaccione a distintos descriptores del sonido como ritmo, espectro, envolvente, etc., utilizando exclusivamente Python.\n",
    "\n",
    "#### Resumen de lo aprendido y conceptos explicados\n",
    "\n",
    "##### Descriptores de audio\n",
    "Hemos sabido comprender y aplicar diferentes descriptores que capturan distintas propiedades del sonido:\n",
    "\n",
    "1. Envolvente de amplitud\n",
    "\n",
    "* Representa la variación de la energía de la señal a lo largo del tiempo.\n",
    "\n",
    "* Útil para detectar intensidad, ataques o silencios.\n",
    "\n",
    "2. Espectro de frecuencias\n",
    "\n",
    "* Se obtiene mediante la Transformada de Fourier.\n",
    "\n",
    "* Permite ver qué frecuencias están presentes en un instante.\n",
    "\n",
    "3. Centroide espectral\n",
    "\n",
    "* Indica el “centro de gravedad” del espectro.\n",
    "\n",
    "* Cuanto más alto, más brillante o agudo se percibe el sonido.\n",
    "\n",
    "4. Autocorrelación\n",
    "\n",
    "* Herramienta para detectar periodicidad o repetición.\n",
    "\n",
    "* Aplicada sobre la envolvente para estimar el ritmo o tempo.\n",
    "\n",
    "5. Detección de beats\n",
    "\n",
    "* Se basa en picos periódicos de energía detectados en la envolvente.\n",
    "\n",
    "* Permite sincronizar efectos visuales con los golpes musicales.\n",
    "\n",
    "##### Procesamiento por fotogramas\n",
    "* El audio se divide en ventanas temporales por fotogramas de vídeo.\n",
    "\n",
    "* Cada fragmento se analiza individualmente para obtener descriptores y generar visuales sincronizados.\n",
    "\n",
    "##### Relación audio → imagen\n",
    "Hemos aprendido a convertir propiedades del audio en parámetros visuales:\n",
    "\n",
    "* Pitch o frecuencia → posición vertical.\n",
    "\n",
    "* Envolvente → tamaño o brillo.\n",
    "\n",
    "* Centroide espectral → color.\n",
    "\n",
    "* Beat → efectos puntuales (flashes, cambios bruscos).\n",
    "\n",
    "Hemos aplicado conceptos de visualización dinámica, donde el vídeo no es estático, sino que evoluciona en función del sonido.\n",
    "\n",
    "##### Programación y diseño de sistema\n",
    "* Hemos optado por un diseño modular del sistema: funciones pequeñas y reutilizables para análisis, extracción de descriptores y visualización.\n",
    "\n",
    "* Hemos hecho uso de librerías como matplotlib, scipy, numpy, colorsys.\n",
    "\n",
    "* Hemos realizado la automatización de video + audio con ffmpeg.\n",
    "\n",
    "#### Funciones creadas\n",
    "\n",
    "* `normalizar(v)`: normaliza valores en rango [0, 1].\n",
    "\n",
    "* `autocorrelacion(x_frame)`: autocorrelación normalizada de un fragmento.\n",
    "\n",
    "* `detectar_ritmo(x_frame, fs, ...)`: calcula el periodo dominante.\n",
    "\n",
    "* `es_beat(frame_index, beat_frames, ...)`: determina si un frame es un beat.\n",
    "\n",
    "* `dibujar_flash()`: dibuja un flash central.\n",
    "\n",
    "* `dibujar_barras(ax, X_resampled, N_BARRAS)`: visualización espectral.\n",
    "\n",
    "* `color_fondo_por_centroide(x_frame, fs)`: color de fondo basado en centroide.\n",
    "\n",
    "* `beat_frames(x, fs)`: calcula los beats a partir de la envolvente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cff21",
   "metadata": {},
   "source": [
    "## PixelSounds: Codec Reversible de Audio y Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f760f61",
   "metadata": {},
   "source": [
    "\n",
    "### Introducción: Codificación de Audio en Vídeo con PixelSounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a7107",
   "metadata": {},
   "source": [
    "PixelSounds es un sistema que convierte una señal de audio en un vídeo compuesto por frames. Cada frame codifica un bloque temporal del audio en forma de imagen, permitiendo visualizar la información sonora a través de representaciones visuales. El proceso es completamente reversible: se puede reconstruir el audio original a partir del vídeo.\n",
    "\n",
    "Este enfoque permite investigar nuevas formas de percepción auditiva basadas en representación visual, y propone un esquema de codificación que, hasta donde sabemos, no está presente en herramientas o artículos convencionales de tratamiento de audio.\n",
    "\n",
    "---\n",
    "\n",
    "### Modos de codificación (`map_mode`)\n",
    "\n",
    "Cada bloque de audio se transforma en una fila de píxeles mediante uno de los siguientes modos:\n",
    "\n",
    "- **`ampl`**  \n",
    "  Codifica la amplitud normalizada del audio en una escala de grises.  \n",
    "  *Este modo ofrece la solución más simple y directa, útil como base de referencia.*\n",
    "\n",
    "- **`fft`**  \n",
    "  Codifica el espectro en frecuencia (magnitud y fase) usando la Transformada Rápida de Fourier (FFT). Requiere codificar dos componentes por muestra.  \n",
    "  *El modo más completo y desafiante de reconstruir, ya que exige preservar fase.*\n",
    "\n",
    "- **`fir`**  \n",
    "  Separa el audio en tres bandas de frecuencia (baja, media y alta) usando filtros FIR, y codifica cada banda en uno de los canales de color (R, G, B).  \n",
    "  *Una solución creativa para dividir el contenido espectral de manera intuitiva.*\n",
    "\n",
    "---\n",
    "\n",
    "### Modos de visualización (`color_mode`)\n",
    "\n",
    "Los frames generados pueden representarse en dos formas:\n",
    "\n",
    "- **`gris`**  \n",
    "  Las filas codificadas se replican verticalmente como una imagen monocroma. Este modo se usa, por ejemplo, para visualizar la amplitud o una sola banda.  \n",
    "  *Una buena opción para mantener fidelidad al contenido numérico.*\n",
    "\n",
    "- **`color`**  \n",
    "  Se colorea cada muestra según su valor en los tres canales RGB, permitiendo ver diferentes componentes (por ejemplo, magnitud y fase, o bandas FIR) en diferentes colores.  \n",
    "  *Este modo explora la intersección entre percepción visual y estructuras del sonido.*\n",
    "\n",
    "---\n",
    "\n",
    "### Estructura del sistema\n",
    "\n",
    "- **Codificador:**  \n",
    "  Hace un análisis por bloques solapados y enventanados del audio, aplica la codificación elegida y genera una secuencia de imágenes PNG. Luego empaqueta los frames en un vídeo MP4 usando FFmpeg.  \n",
    "\n",
    "- **Decodificador:**  \n",
    "  Extrae los frames del vídeo, reconstruye los bloques de audio a partir de las imágenes, y aplica overlap-add para recuperar la señal completa.  \n",
    "  *Permite probar si la codificación elegida conserva adecuadamente la señal.*\n",
    "\n",
    "---\n",
    "\n",
    "Esta estructura modular permite experimentar con distintas formas de representar el contenido sonoro, y abre la puerta a posibles usos en visualización interactiva, arte sonoro o codificación alternativa. Aunque centrado en una tarea técnica, el sistema introduce elementos creativos que conectan con otras disciplinas.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd82afa",
   "metadata": {},
   "source": [
    "\n",
    "### Detalles del flujo del codificador\n",
    "\n",
    "El proceso de codificación se inicia dividiendo el audio en bloques de tamaño fijo `N = fs / fps`, aplicando una ventana para suavizar los bordes, y generando una imagen por cada bloque.\n",
    "\n",
    "Antes de comenzar con los datos, el sistema crea un **primer frame especial que contiene los metadatos** necesarios para la decodificación. Este frame de encabezado ocupa la posición `frame_0000.png` y codifica, en sus primeras filas, información crítica como:\n",
    "\n",
    "- Tamaño del bloque `N` (dos bytes)\n",
    "- Tamaño del salto `hop` entre bloques\n",
    "- Frecuencia de vídeo `fps`\n",
    "- Modo de visualización (`0` para gris, `1` para color)\n",
    "\n",
    "Gracias a este diseño, no es necesario acompañar el vídeo de archivos auxiliares o configuraciones externas: todo lo necesario para la reconstrucción está embebido en el propio vídeo.\n",
    "\n",
    "Después, para cada bloque de audio:\n",
    "\n",
    "1. Se calcula la fila base de píxeles aplicando el modo de codificación (`map_mode`).\n",
    "2. Dependiendo del modo de visualización (`color_mode`), esa fila se convierte en una imagen monocroma o en una representación coloreada.\n",
    "3. Cada imagen se guarda como un fotograma numerado secuencialmente (`frame_0001.png`, `frame_0002.png`, etc.).\n",
    "4. Finalmente, todas las imágenes se empaquetan en un archivo `.mp4` con ffmpeg.\n",
    "\n",
    "Este flujo permite una representación audiovisual compacta, autosuficiente y fácilmente reversible del audio original.\n",
    "\n",
    "Para garantizar que no haya pérdida de información entre los frames PNG generados y el vídeo final `.mp4`, el empaquetado se realiza con los siguientes parámetros de codificación:\n",
    "\n",
    "- **Calidad constante sin pérdidas visuales:** se usa `-crf 0` o valores muy bajos para mantener fidelidad.\n",
    "- **Submuestreo desactivado (4:4:4):** se fuerza `pix_fmt=yuv444p` para evitar la compresión cromática que degradaría los valores de color en `color_mode`.\n",
    "- **Compresión intra-frame:** se evita la predicción entre frames, lo que permite acceder a cualquier frame de forma independiente sin depender de otros.\n",
    "\n",
    "Esto asegura que el vídeo generado conserva exactamente los valores de píxeles de cada imagen, lo que es fundamental para que la reconstrucción del audio sea precisa y consistente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b058f98",
   "metadata": {},
   "source": [
    "> ⚠️ **Advertencia sobre tamaño y reproducción de vídeo**  \n",
    "> La generación de vídeo con PixelSounds puede producir archivos **grandes** (especialmente en modo color y `fft`). Además, el número de frames por segundo (`fps`) y la compresión sin pérdidas hacen que algunos reproductores no gestionen bien el resultado.\n",
    ">\n",
    "> Para una visualización fluida y precisa, se recomienda abrir los vídeos generados con **VLC Media Player** o un reproductor avanzado que soporte bien vídeo con:\n",
    ">\n",
    "> - Compresión `libx264` sin pérdidas (`-crf 0`)\n",
    "> - Formato de color **yuv444p** (sin submuestreo)\n",
    "> - Muchos fotogramas por segundo (por ejemplo, `fps=60`)\n",
    ">\n",
    "> En algunos casos, la previsualización desde el explorador o el propio notebook puede fallar o ir a saltos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e83c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio.v2 as imageio\n",
    "from scipy.signal import firwin, lfilter, get_window\n",
    "from scipy.fft import fft, ifft\n",
    "from scipy.io import wavfile as wav\n",
    "from IPython.display import Audio, Video, display\n",
    "\n",
    "from vozyaudio import lee_audio, sonido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelSoundsEncoder:\n",
    "    \"\"\"\n",
    "    Convierte un audio en un “vídeo pixelado” codificando cada bloque de\n",
    "    muestras en filas de imágenes.\n",
    "\n",
    "    - Primer frame: metadatos (N, hop, fps, color_mode).\n",
    "    - Siguientes frames: bloques de audio coloreados (gris o RGB).\n",
    "    - Luego empaqueta PNGs en MP4 con un batch externo.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_path,\n",
    "        frames_dir=\"fotogramas\",\n",
    "        export_dir=\"exports\",\n",
    "        fps=60,\n",
    "        color_mode=\"color\",   # 'gris' o 'color'\n",
    "        map_mode=\"ampl\",       # 'ampl', 'fft' o 'fir'\n",
    "        window_type=\"hann\",\n",
    "        numcoef=101\n",
    "    ):\n",
    "        self.audio_path  = audio_path\n",
    "        self.frames_dir  = frames_dir\n",
    "        self.export_dir  = export_dir\n",
    "        self.fps         = fps\n",
    "        self.color_mode  = color_mode\n",
    "        self.map_mode    = map_mode\n",
    "        self.window_type = window_type\n",
    "        self.iscolor     = 1 if color_mode == \"color\" else 0\n",
    "\n",
    "        # Leer y normalizar audio de 16-bit\n",
    "        self.fs, audio = lee_audio(audio_path)\n",
    "        self.audio     = audio.astype(np.float32)\n",
    "        self.audio    /= np.max(np.abs(self.audio)) + 1e-12\n",
    "\n",
    "        # Diseñar filtros FIR\n",
    "        self.b_low  = firwin(numcoef,       cutoff=3000,                   fs=self.fs)\n",
    "        self.b_band = firwin(numcoef, [3000,10000], pass_zero=False,         fs=self.fs)\n",
    "        self.b_high = firwin(numcoef,       cutoff=10000, pass_zero=False,   fs=self.fs)\n",
    "\n",
    "        # Crear carpetas de salida\n",
    "        if os.path.exists(self.frames_dir):\n",
    "            shutil.rmtree(self.frames_dir)\n",
    "        os.makedirs(self.frames_dir, exist_ok=True)\n",
    "        os.makedirs(self.export_dir, exist_ok=True)\n",
    "\n",
    "    def _write_header(self, N, hop):\n",
    "        \"\"\"\n",
    "        Escribe el frame 0000 como cabecera con metadatos de N, hop, fps y color_mode.\n",
    "        \"\"\"\n",
    "        # crear imagen vacia NxN\n",
    "        header = np.zeros((N, N), dtype=np.uint8)\n",
    "\n",
    "        # codificar N y hop en bytes alto y bajo\n",
    "        hiN, loN = (N>>8)&0xFF, N&0xFF\n",
    "        hiH, loH = (hop>>8)&0xFF, hop&0xFF\n",
    "\n",
    "        # rellenar filas con metadatos\n",
    "        header[0,:] = hiN\n",
    "        header[1,:] = loN\n",
    "        header[2,:] = hiH\n",
    "        header[3,:] = loH\n",
    "        header[4,:] = self.fps & 0xFF         # fps\n",
    "        header[5,:] = self.iscolor            # flag color\n",
    "\n",
    "        # guardar como frame_0000\n",
    "        path = os.path.join(self.frames_dir, \"frame_0000.png\")\n",
    "        imageio.imwrite(path, header)\n",
    "\n",
    "    def _colorear_fila(self, fila, modo):\n",
    "        \"\"\"\n",
    "        Aplica mapeado de color a la fila segun el modo.\n",
    "\n",
    "        - ampl → R = amp, G = 255-amp, B = constante\n",
    "        - fft  → R = mag, G = phase, B = constante\n",
    "        - fir  → R/G/B = bandas low/band/high\n",
    "\n",
    "        Devuelve un array Nx3 uint8 (RGB por pixel).\n",
    "        \"\"\"\n",
    "\n",
    "        if modo == 'ampl':\n",
    "            # si es color, cogemos solo el canal rojo\n",
    "            amp = fila[:,0] if fila.ndim == 2 else fila\n",
    "\n",
    "            # rojo: valor original\n",
    "            r = amp\n",
    "            # verde: complementario para dar contraste\n",
    "            g = 255 - amp\n",
    "            # azul: constante (centro)\n",
    "            b = 128 * np.ones_like(amp, dtype=np.uint8)\n",
    "\n",
    "        elif modo == 'fft':\n",
    "            # usamos magnitud (R) y fase (G), B fijo\n",
    "            mag_n, phase_n = fila[:,0], fila[:,1]\n",
    "            r = mag_n\n",
    "            g = phase_n\n",
    "            b = 255 * np.ones_like(r, dtype=np.uint8)  # canal azul a tope\n",
    "\n",
    "        elif modo == 'fir':\n",
    "            # ya vienen como low/band/high → R/G/B\n",
    "            r, g, b = fila[:,0], fila[:,1], fila[:,2]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Modo desconocido para colorear: {modo}\")\n",
    "\n",
    "        # ensamblar los tres canales en un solo array (Nx3)\n",
    "        return np.stack([r, g, b], axis=1).astype(np.uint8)\n",
    "\n",
    "    def generate_frames(self):\n",
    "        \"\"\"\n",
    "        Divide el audio en bloques, aplica codificacion y guarda cada uno como PNG.\n",
    "        \"\"\"\n",
    "        # 1) Calcular tamaño de bloque y hop\n",
    "        N   = self.fs // self.fps\n",
    "        if N % 2: N += 1               # aseguramos par\n",
    "        hop = N // 2\n",
    "\n",
    "        # 2) Guardar cabecera\n",
    "        self._write_header(N, hop)\n",
    "\n",
    "        # 3) Ventana y num de bloques\n",
    "        window   = get_window(self.window_type, N, fftbins=True)\n",
    "        n_blocks = (len(self.audio) - N) // hop\n",
    "        print(f\"[Encoder] N={N}, HOP={hop}, FPS={self.fps}, Bloques={n_blocks}\")\n",
    "\n",
    "        # 4) Procesar bloque a bloque\n",
    "        for i in range(n_blocks):\n",
    "            start = i * hop\n",
    "            block = self.audio[start:start+N] * window\n",
    "\n",
    "            # 4.1) Codificar segun map_mode\n",
    "            if self.map_mode == \"ampl\":\n",
    "                # normalizar el bloque a [0,1] y escalar a 8 bits\n",
    "                norm = (block - block.min()) / (block.max() - block.min() + 1e-12)\n",
    "                fila = (norm * 255).astype(np.uint8)  # fila resultante, uint8\n",
    "\n",
    "            elif self.map_mode == \"fft\":\n",
    "                # calcular la FFT\n",
    "                X       = fft(block, n=N)\n",
    "                X       = np.fft.fftshift(X)          # centrar la FFT\n",
    "                mag     = np.abs(X)                   # magnitud\n",
    "                phase   = np.angle(X)                 # fase en radianes\n",
    "\n",
    "                # normalizar magnitud a [0,255]\n",
    "                mag_n   = np.round((mag / (mag.max() + 1e-12)) * 255).astype(np.uint8)\n",
    "                # convertir fase de [-pi, pi] → [0, 255]\n",
    "                phase_n = np.round(((phase + np.pi) / (2*np.pi)) * 255).astype(np.uint8)\n",
    "\n",
    "                # construir fila RGB: R=mag, G=phase, B=0\n",
    "                fila    = np.stack([mag_n, phase_n, np.zeros_like(mag_n)], axis=1)\n",
    "\n",
    "            elif self.map_mode == \"fir\":\n",
    "                # aplicar 3 filtros FIR: low, band y high\n",
    "                y_l = lfilter(self.b_low,  1.0, block)\n",
    "                y_b = lfilter(self.b_band, 1.0, block)\n",
    "                y_h = lfilter(self.b_high, 1.0, block)\n",
    "\n",
    "                # recortar cada señal a [-1,1]\n",
    "                y_l, y_b, y_h = map(lambda y: np.clip(y, -1, 1), (y_l, y_b, y_h))\n",
    "\n",
    "                # convertir a int8 → uint8 (para guardar como imagen)\n",
    "                r8 = np.round(y_l * 127).astype(np.int8).view(np.uint8)\n",
    "                g8 = np.round(y_b * 127).astype(np.int8).view(np.uint8)\n",
    "                b8 = np.round(y_h * 127).astype(np.int8).view(np.uint8)\n",
    "\n",
    "                # construir fila RGB\n",
    "                fila = np.stack([r8, g8, b8], axis=1)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"map_mode debe ser 'ampl', 'fft' o 'fir'\")\n",
    "\n",
    "\n",
    "            # 4.2) Construir imagen segun color_mode\n",
    "            if self.color_mode == \"gris\":\n",
    "                if self.map_mode == \"fft\":\n",
    "                    # intercalar mag y phase\n",
    "                    img = np.empty((N, N), dtype=np.uint8)\n",
    "                    img[0::2, :] = mag_n[np.newaxis, :]\n",
    "                    img[1::2, :] = phase_n[np.newaxis, :]\n",
    "                elif self.map_mode == \"fir\":\n",
    "                    # intercalar bandas y repetir verticalmente\n",
    "                    img = np.empty((N, N), dtype=np.uint8)\n",
    "                    img[0::3, :] = r8[np.newaxis, :]\n",
    "                    img[1::3, :] = g8[np.newaxis, :]\n",
    "                    img[2::3, :] = b8[np.newaxis, :]\n",
    "                else:\n",
    "                    gris = fila if fila.ndim == 1 else fila[:,0]\n",
    "                    img  = np.tile(gris[np.newaxis,:], (N,1))\n",
    "            else:\n",
    "                # modo color\n",
    "                base_rgb = fila if fila.ndim == 2 else np.stack([fila]*3, axis=1)\n",
    "                colored  = self._colorear_fila(base_rgb, self.map_mode)\n",
    "                img      = np.tile(colored[np.newaxis,...], (N,1,1))\n",
    "\n",
    "            # 4.3) Guardar imagen PNG\n",
    "            path = os.path.join(self.frames_dir, f\"frame_{i+1:04d}.png\")\n",
    "            imageio.imwrite(path, img)\n",
    "\n",
    "        print(f\"[Encoder] Frames generados en '{self.frames_dir}/'\")\n",
    "\n",
    "\n",
    "    def encode_video(self, output_name=None):\n",
    "        \"\"\"\n",
    "        Empaqueta los frames PNG como un MP4 sin pérdida y añade el audio reconstruido si está disponible.\n",
    "\n",
    "        - Video: libx264, CRF 0, YUV444p\n",
    "        - Audio (opcional): reconstruido desde generate_frames, codificado como AAC\n",
    "        \"\"\"\n",
    "        base = os.path.splitext(os.path.basename(self.audio_path))[0]\n",
    "        name = output_name or f\"{base}_{self.map_mode}_{self.color_mode}.mp4\"\n",
    "        out_video = os.path.join(self.export_dir, name)\n",
    "\n",
    "        # Eliminar si ya existe\n",
    "        if os.path.exists(out_video):\n",
    "            os.remove(out_video)\n",
    "\n",
    "        # Paso 1: Generar MP4 a partir de imágenes (sin audio aún)\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\",\n",
    "            \"-framerate\", str(self.fps),\n",
    "            \"-i\", os.path.join(self.frames_dir, \"frame_%04d.png\"),\n",
    "            \"-c:v\", \"libx264\",\n",
    "            \"-crf\", \"0\",\n",
    "            \"-preset\", \"veryslow\",\n",
    "            \"-pix_fmt\", \"yuv444p\",\n",
    "            out_video\n",
    "        ]\n",
    "        subprocess.run(cmd, check=True)\n",
    "        print(f\"[Encoder] Video sin audio exportado en '{out_video}'\")\n",
    "\n",
    "        # Paso 2: Añadir audio reconstruido si existe\n",
    "        if hasattr(self, \"_recon_audio\") and self._recon_audio is not None:\n",
    "            temp_wav = os.path.join(self.export_dir, \"temp_audio.wav\")\n",
    "            wav.write(temp_wav, self.fs, (self._recon_audio * 32767).astype(np.int16))\n",
    "\n",
    "            out_final = out_video.replace(\".mp4\", \"_withaudio.mp4\")\n",
    "\n",
    "            cmd_audio = [\n",
    "                \"ffmpeg\", \"-y\",\n",
    "                \"-i\", out_video,\n",
    "                \"-i\", temp_wav,\n",
    "                \"-c:v\", \"copy\",\n",
    "                \"-c:a\", \"aac\",  # o \"pcm_s16le\" si prefieres sin compresión\n",
    "                \"-shortest\",\n",
    "                out_final\n",
    "            ]\n",
    "            subprocess.run(cmd_audio, check=True)\n",
    "            print(f\"[Encoder] Video final con audio reconstruido: '{out_final}'\")\n",
    "\n",
    "            return out_final\n",
    "\n",
    "        return out_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelSoundsDecoder:\n",
    "    \"\"\"\n",
    "    Decodifica un vídeo generado por PixelSoundsEncoder de vuelta a WAV.\n",
    "\n",
    "    - Primer frame: metadatos (N, hop, fps, color_mode).\n",
    "    - Siguientes frames: bloques de audio (gris o RGB) codificados.\n",
    "    - Reconstruye por overlap-add y guarda WAV.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        frames_dir,\n",
    "        output_wav,\n",
    "        map_mode='ampl',    # 'ampl', 'fft' o 'fir'\n",
    "        window_type='hann'  # tipo de ventana para overlap-add\n",
    "    ):\n",
    "        self.frames_dir  = frames_dir\n",
    "        self.output_wav  = output_wav\n",
    "        self.map_mode    = map_mode\n",
    "        self.window_type = window_type\n",
    "\n",
    "    def extract_all_frames(self, video_path, prefix=\"frame_\", fmt=\"png\"):\n",
    "        \"\"\"\n",
    "        Extrae todos los frames de un vídeo y los guarda como imágenes PNG numeradas\n",
    "        en la carpeta self.frames_dir. Limpia el contenido previo si existe.\n",
    "\n",
    "        Parámetros:\n",
    "        - video_path: ruta al vídeo del que extraer los frames\n",
    "        - prefix: prefijo para nombrar los archivos generados\n",
    "        - fmt: formato de imagen de salida (por defecto 'png')\n",
    "        \"\"\"\n",
    "        print(f\"[Decoder] Extrayendo frames de '{video_path}'...\")\n",
    "\n",
    "        # si existe la carpeta, la limpiamos por completo\n",
    "        if os.path.exists(self.frames_dir):\n",
    "            shutil.rmtree(self.frames_dir)\n",
    "        os.makedirs(self.frames_dir, exist_ok=True)\n",
    "\n",
    "        # abrimos el vídeo\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise IOError(f\"No se pudo abrir el vídeo {video_path}\")\n",
    "\n",
    "        # bucle para leer y guardar todos los frames\n",
    "        idx = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()           # intentamos leer el siguiente frame\n",
    "            if not ret:\n",
    "                break                         # fin del vídeo\n",
    "            fname = f\"{prefix}{idx:04d}.{fmt}\"  # frame_0000.png, frame_0001.png, ...\n",
    "            cv2.imwrite(os.path.join(self.frames_dir, fname), frame)\n",
    "            idx += 1\n",
    "\n",
    "        cap.release()\n",
    "        print(f\"[Decoder] Total frames extraídos: {idx}\")\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def extraer_metadatos_cabecera_rows(self, frame):\n",
    "        \"\"\"Lee la cabecera y extrae N, hop, fps y si está en color\"\"\"\n",
    "        print(\"[Decoder] Leyendo metadatos de header...\")\n",
    "        if frame.dtype != np.uint8:\n",
    "            frame = np.clip(frame * 255, 0, 255).astype(np.uint8)\n",
    "        if frame.ndim == 3:\n",
    "            frame = frame[..., 0]  # usar solo canal rojo si es RGB\n",
    "\n",
    "        # extraer bytes altos y bajos para N y hop\n",
    "        hiN, loN = int(frame[0,0]), int(frame[1,0])\n",
    "        hiH, loH = int(frame[2,0]), int(frame[3,0])\n",
    "        fps      = int(frame[4,0])\n",
    "        flag     = int(frame[5,0])\n",
    "\n",
    "        N        = (hiN << 8) | loN\n",
    "        hop      = (hiH << 8) | loH\n",
    "        is_color = bool(flag)\n",
    "\n",
    "        print(f\"[Decoder] Header -> N={N}, hop={hop}, fps={fps}, modo={'color' if is_color else 'gris'}\")\n",
    "        return N, hop, fps, is_color\n",
    "\n",
    "    def decode(self):\n",
    "        \"\"\"\n",
    "        Reconstruye el audio a partir de los PNG ya extraídos en self.frames_dir.\n",
    "        Asume que frame_0000.png (header) y los frames de datos están presentes.\n",
    "        \"\"\"\n",
    "        # 1) Leer metadatos desde el header\n",
    "        header = imageio.imread(os.path.join(self.frames_dir, \"frame_0000.png\"))\n",
    "        N, hop, fps, self.is_color = self.extraer_metadatos_cabecera_rows(header)\n",
    "        fs_recon = N * fps\n",
    "\n",
    "        # 2) Obtener listado de frames ignorando el header\n",
    "        files   = sorted(f for f in os.listdir(self.frames_dir)\n",
    "                        if f.startswith(\"frame_\") and f != \"frame_0000.png\")\n",
    "        n_blocks = len(files)\n",
    "\n",
    "        # 3) Inicializar buffers para reconstrucción por superposición (overlap-add)\n",
    "        length  = N + hop * (n_blocks - 1)\n",
    "        audio   = np.zeros(length, dtype=np.float32)\n",
    "        pesos   = np.zeros(length, dtype=np.float32)\n",
    "        ventana = get_window(self.window_type, N, fftbins=True)\n",
    "\n",
    "        # 4) Decodificar bloque a bloque\n",
    "        for i, fname in enumerate(files, start=1):\n",
    "            path = os.path.join(self.frames_dir, fname)\n",
    "            raw  = imageio.imread(path)  # imagen uint8\n",
    "\n",
    "            if self.is_color:\n",
    "                # === Modo color ===\n",
    "                row_uint8 = raw[0]  # usamos solo la primera fila\n",
    "\n",
    "                if self.map_mode == 'ampl':\n",
    "                    amp_norm = row_uint8[:,0].astype(np.float32) / 255.0\n",
    "                    block    = amp_norm * 2.0 - 1.0\n",
    "\n",
    "                elif self.map_mode == 'fft':\n",
    "                    mag_n    = row_uint8[:,0].astype(np.float32) / 255.0\n",
    "                    phase_n  = (row_uint8[:,1].astype(np.float32) / 255.0) * 2*np.pi - np.pi\n",
    "                    X        = mag_n * np.exp(1j * phase_n)\n",
    "                    X        = np.fft.ifftshift(X)\n",
    "                    block    = np.real(ifft(X, n=N))\n",
    "\n",
    "                elif self.map_mode == 'fir':\n",
    "                    # cada canal representa una banda: low, band, high\n",
    "                    pix_c = np.ascontiguousarray(row_uint8)\n",
    "                    y_l   = pix_c[:,0].view(np.int8).astype(np.float32) / 127.0\n",
    "                    y_b   = pix_c[:,1].view(np.int8).astype(np.float32) / 127.0\n",
    "                    y_h   = pix_c[:,2].view(np.int8).astype(np.float32) / 127.0\n",
    "                    block = y_l + y_b + y_h\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"map_mode desconocido: {self.map_mode}\")\n",
    "\n",
    "            else:\n",
    "                # === Modo gris ===\n",
    "                gray = raw[...,0] if raw.ndim == 3 else raw\n",
    "\n",
    "                if self.map_mode == 'ampl':\n",
    "                    pix   = gray[0].astype(np.float32)\n",
    "                    block = (pix / 255.0) * 2.0 - 1.0\n",
    "\n",
    "                elif self.map_mode == 'fft':\n",
    "                    mag_n    = gray[0, :].astype(np.float32) / 255.0\n",
    "                    phase_n  = (gray[1, :].astype(np.float32) / 255.0) * 2*np.pi - np.pi\n",
    "                    X        = mag_n * np.exp(1j * phase_n)\n",
    "                    X        = np.fft.ifftshift(X)\n",
    "                    block    = np.real(ifft(X, n=N))\n",
    "\n",
    "                elif self.map_mode == 'fir':\n",
    "                    # filas intercaladas 0::3,1::3,2::3\n",
    "                    y_l = gray[0::3, :][0].view(np.int8).astype(np.float32) / 127.0\n",
    "                    y_b = gray[1::3, :][0].view(np.int8).astype(np.float32) / 127.0\n",
    "                    y_h = gray[2::3, :][0].view(np.int8).astype(np.float32) / 127.0\n",
    "                    # recombinar bandas\n",
    "                    block = y_l + y_b + y_h\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"map_mode desconocido: {self.map_mode}\")\n",
    "\n",
    "            # 4.2) Superposición por ventana\n",
    "            start = (i-1) * hop\n",
    "            audio[start:start+N] += block * ventana\n",
    "            pesos[start:start+N] += ventana\n",
    "\n",
    "        # 5) Normalizar por ventana y guardar\n",
    "        audio /= (pesos + 1e-12)\n",
    "        sonido(audio, fs_recon)  # opcional: reproducir\n",
    "        os.makedirs(os.path.dirname(self.output_wav) or '.', exist_ok=True)\n",
    "        scaled = np.int16(np.clip(audio, -1, 1) * 32767)\n",
    "        wav.write(self.output_wav, fs_recon, scaled)\n",
    "\n",
    "        return audio, fs_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in (\"fotogramas\", \"exports\"):\n",
    "    if os.path.exists(d):\n",
    "        print(f\"Eliminando carpeta existente: {d}/\")\n",
    "        shutil.rmtree(d)\n",
    "    else:\n",
    "        print(f\"No existe: {d}/ — nada que borrar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1658c",
   "metadata": {},
   "source": [
    "### 1. Carga de audio\n",
    "\n",
    "Cargamos un WAV de ejemplo y mostramos sus primeros segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"audios\\music.wav\"\n",
    "\n",
    "fs, orig = lee_audio(audio_path)\n",
    "print(f\"Frecuencia de muestreo: {fs} Hz\")\n",
    "Audio(orig, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a888ce",
   "metadata": {},
   "source": [
    "### 2. Definición de parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148e2eb",
   "metadata": {},
   "source": [
    "### Relación entre FPS y FS en PixelSounds\n",
    "\n",
    "Una parte clave del sistema PixelSounds es la relación entre la frecuencia de muestreo del audio (`fs`) y los fotogramas por segundo (`fps`) del vídeo generado. Esta relación determina cómo se divide la señal de audio en bloques temporales y, por tanto, qué resolución temporal tendrá la visualización.\n",
    "\n",
    "#### Conversión audio → vídeo\n",
    "\n",
    "- El audio original tiene una frecuencia de muestreo `fs` (por ejemplo, 44100 Hz).\n",
    "- El vídeo tendrá `fps` (por ejemplo, 60 fotogramas por segundo).\n",
    "- Cada frame del vídeo representará un bloque de **N = fs / fps** muestras de audio.\n",
    "\n",
    "> Por ejemplo, si `fs = 44100` Hz y `fps = 60`, entonces `N = 735`.  \n",
    "> Esto significa que cada fotograma representa 735 muestras de audio, es decir, 735 / 44100 = 0.0166 segundos ≈ 16.6 ms de sonido.\n",
    "\n",
    "#### Ventaneo y solapamiento\n",
    "\n",
    "- Para evitar discontinuidades, se utiliza un solapamiento entre bloques con salto `hop = N // 2` (solapamiento del 50%).\n",
    "- Cada bloque se multiplica por una ventana (por ejemplo, tipo `hann`) antes de ser codificado para minimizar artefactos.\n",
    "\n",
    "#### Efecto visual y auditivo\n",
    "\n",
    "- Un valor alto de `fps` produce más frames por segundo y, por tanto, bloques de audio más pequeños (mayor resolución temporal, más precisión visual).\n",
    "- Un valor bajo de `fps` produce menos frames por segundo y bloques más grandes (más eficiencia, pero menor detalle en la visualización).\n",
    "\n",
    "> Es crucial mantener `fs` y `fps` constantes durante todo el proceso para asegurar que la reconstrucción del audio a partir del vídeo sea coherente y sin errores.\n",
    "\n",
    "Esta correspondencia directa entre parámetros típicos del mundo audiovisual (`fps`) y el dominio de la señal (`fs`) no solo favorece una comprensión intuitiva de la codificación, sino que también resalta cómo el diseño del sistema integra decisiones técnicas con consecuencias visuales y auditivas. Elegir una `fps` adecuada es, por tanto, un equilibrio entre estética visual y fidelidad sonora.\n",
    "\n",
    "Además, el uso del solapamiento y la ventana aplicada a cada bloque no es meramente un detalle técnico: refleja una comprensión precisa de cómo minimizar discontinuidades entre bloques, técnica comúnmente usada en transformadas de corto tiempo o en codificadores perceptuales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps         = 60\n",
    "window_type = \"hann\"  # otras opciones: \"hamming\", \"blackman\", \"bartlett\", \"kaiser\", \"boxcar\", \"triang\", \"nuttall\", \"flattop\", \"parzen\", \"bohman\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b01177",
   "metadata": {},
   "source": [
    "### 3. Modo Amplitude (`ampl`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8b8bd",
   "metadata": {},
   "source": [
    "\n",
    "### Codificación\n",
    "\n",
    "- **Bloque y ventana**  \n",
    "  Se extrae un fragmento de \\( N \\) muestras del audio y se aplica una ventana (por ejemplo, Hann) para evitar discontinuidades en los bordes.\n",
    "\n",
    "- **Normalización a [0–1]**  \n",
    "  $$\n",
    "  \\text{norm} = \\frac{\\text{block} - \\min(\\text{block})}{\\max(\\text{block}) - \\min(\\text{block}) + \\varepsilon}\n",
    "  $$  \n",
    "  Esto asegura que la amplitud mínima del bloque mapee a 0 y la máxima a 1.\n",
    "\n",
    "- **Cuantización a 8 bits**  \n",
    "  $$\n",
    "  \\text{fila}_i = \\lfloor \\text{norm}_i \\times 255 \\rfloor \\quad \\text{con} \\quad 0 \\leq \\text{fila}_i \\leq 255\n",
    "  $$\n",
    "\n",
    "- **Construcción de la imagen**  \n",
    "  - **Escala de grises**: se replica la misma fila de \\( N \\) píxeles en cada una de las \\( N \\) filas del PNG, resultando en una imagen uniforme.\n",
    "  - **Modo color**:  \n",
    "    $$\n",
    "    R = \\text{amplitud}, \\quad G = 255 - \\text{amplitud}, \\quad B = 128\n",
    "    $$\n",
    "\n",
    "> Aunque este modo es el más sencillo, no se ha descuidado su implementación: se asegura una normalización por bloque que preserva el rango dinámico local, una cuantización sin ambigüedad en 8 bits, y una representación visual coherente y fácilmente reversible. Esta atención al detalle permite que incluso esta versión básica sirva como referencia estable para comparar con las codificaciones más complejas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar → Empaquetar → Decodificar para ampl (gris y color)\n",
    "map_mode = \"ampl\"\n",
    "for color in (\"gris\", \"color\"):\n",
    "    # Carpetas de salida\n",
    "    frames_dir = f\"fotogramas/{map_mode}_{color}_fotogramas\"\n",
    "    export_dir = f\"exports/{map_mode}_{color}\"\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # CODIFICAR\n",
    "    enc = PixelSoundsEncoder(\n",
    "        audio_path=audio_path,\n",
    "        frames_dir=frames_dir,\n",
    "        export_dir=export_dir,\n",
    "        fps=fps,\n",
    "        color_mode=color,\n",
    "        map_mode=map_mode,\n",
    "        window_type=window_type\n",
    "    )\n",
    "    enc.generate_frames()\n",
    "\n",
    "    #  EMPAQUETAR VÍDEO\n",
    "    video_file = enc.encode_video()\n",
    "    print(f\"[Notebook] Vídeo generado en: {video_file}\")\n",
    "    display(Video(video_file, embed=True, width=480, height=360))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e04cdc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Decodificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919c520",
   "metadata": {},
   "source": [
    "\n",
    "- **Lectura de píxeles**  \n",
    "  Se carga el PNG y se extrae la fila 0:  \n",
    "  - En gris → valor único por píxel  \n",
    "  - En color → canal Rojo\n",
    "\n",
    "- **Desnormalización a [0–1]**  \n",
    "  $$\n",
    "  \\text{amp} = \\frac{\\text{pixel}}{255}\n",
    "  $$\n",
    "\n",
    "- **Recuperar bipolaridad [–1 … +1]**  \n",
    "  $$\n",
    "  \\text{block}_i = \\text{amp}_i \\times 2 - 1\n",
    "  $$\n",
    "\n",
    "- **Overlap-add con ventana**  \n",
    "  Cada bloque reconstruido se solapa a la mitad (hop = \\( N/2 \\)) y se suma usando la misma ventana, recomponiendo la señal continua.\n",
    "\n",
    "- **Normalización final**  \n",
    "  $$\n",
    "  \\text{audio}[t] \\mathrel{{/}{=}} \\sum \\text{ventana}\n",
    "  $$\n",
    "\n",
    "El resultado es un archivo `.wav` cuya forma de onda sigue fielmente la envolvente original, con las únicas pérdidas debidas a la cuantización a 8 bits y al solapamiento de ventanas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_mode = \"ampl\"\n",
    "base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "\n",
    "for color in (\"gris\", \"color\"):\n",
    "    # 1) Directorios y rutas\n",
    "    frames_dir = f\"fotogramas/{map_mode}_{color}_fotogramas\"\n",
    "    export_dir = f\"exports/{map_mode}_{color}\"\n",
    "    # Nombre del vídeo que acabamos de generar\n",
    "    video_file = os.path.join(export_dir, f\"{base_name}_{map_mode}_{color}.mp4\")\n",
    "    # Fichero WAV de salida\n",
    "    output_wav = os.path.join(export_dir, f\"recon_{base_name}_{map_mode}_{color}.wav\")\n",
    "    \n",
    "    # 2) Instanciar decoder\n",
    "    dec = PixelSoundsDecoder(\n",
    "        frames_dir=frames_dir, # Donde guardar los frames extraidos\n",
    "        output_wav=output_wav, # Video del que sacar los frames\n",
    "        map_mode=map_mode,     # Modo\n",
    "        window_type=window_type # Tipo de ventana\n",
    "    )\n",
    "    \n",
    "    # 3) Extraer frames desde el MP4\n",
    "    dec.extract_all_frames(video_file)\n",
    "    \n",
    "    # 4) Reconstruir el audio y guardarlo\n",
    "    audio_rec, fs_rec = dec.decode()\n",
    "    \n",
    "    # 5) Mostrar y reproducir inline\n",
    "    print(f\"[Notebook] Audio reconstruido ({map_mode}_{color}):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d82841",
   "metadata": {},
   "source": [
    "### 4. Modo FFT (`fft`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aab894",
   "metadata": {},
   "source": [
    "\n",
    "### Codificación\n",
    "\n",
    "- **Transformada de Fourier (FFT)**  \n",
    "  Se aplica una FFT al bloque de audio con longitud \\( N \\) y se centra el espectro usando un `fftshift`.\n",
    "\n",
    "  $$\n",
    "  X = \\text{fftshift}\\left( \\text{FFT}(\\text{block}) \\right)\n",
    "  $$\n",
    "\n",
    "- **Extracción de magnitud y fase**  \n",
    "  $$\n",
    "  \\text{mag} = |X| \\qquad \\text{phase} = \\angle X\n",
    "  $$\n",
    "\n",
    "- **Normalización a enteros sin signo de 8 bits**  \n",
    "  - Magnitud:\n",
    "\n",
    "    $$\n",
    "    \\text{mag}_n = \\left\\lfloor \\frac{\\text{mag}}{\\max(\\text{mag}) + \\varepsilon} \\times 255 \\right\\rfloor\n",
    "    $$\n",
    "\n",
    "  - Fase (rango [\\(-\\pi\\), \\(+\\pi\\)] → [0, 255]):\n",
    "\n",
    "    $$\n",
    "    \\text{phase}_n = \\left\\lfloor \\frac{\\text{phase} + \\pi}{2\\pi} \\times 255 \\right\\rfloor\n",
    "    $$\n",
    "\n",
    "- **Construcción de la imagen**  \n",
    "  - **Escala de grises**:  \n",
    "    Se intercalan las filas de magnitud y fase:\n",
    "\n",
    "    $$\n",
    "    \\text{img}_{2k} = \\text{mag}_n \\quad , \\quad \\text{img}_{2k+1} = \\text{phase}_n\n",
    "    $$\n",
    "\n",
    "  - **Modo color**:  \n",
    "    Se asigna:\n",
    "\n",
    "    $$\n",
    "    R = \\text{mag}_n, \\quad G = \\text{phase}_n, \\quad B = 255\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "> Este modo introduce una complejidad adicional al tener que codificar dos componentes por muestra: magnitud y fase. La elección de normalizar ambas por separado y mapearlas a 8 bits permite mantener la precisión relativa en cada bloque sin necesidad de almacenar escalas globales. El hecho de que la fase esté centrada en cero y remapeada al rango [0, 255] permite una reconstrucción directa desde la imagen, lo cual es poco habitual en representaciones visuales de espectros.\n",
    "\n",
    "> Además, asignar `R = magnitud`, `G = fase` y `B = 255` en modo color no solo es una decisión práctica sino también visualmente significativa: la magnitud domina la percepción de intensidad, mientras que la fase introduce variaciones suaves en el color, facilitando una inspección visual cualitativa del contenido espectral.\n",
    "\n",
    "\n",
    "> Accidentalmente, el desarrollo del sistema comenzó sin incluir la fase: solo se codificaba la magnitud del espectro. Esto permitía una reconstrucción parcial del audio, pero el resultado sonoro era artificial y con un timbre metálico característico. Sin saberlo, estábamos recreando el efecto robótico que se explicó en clase al hablar de la importancia de la fase en la percepción del habla y los timbres naturales.\n",
    "\n",
    "> Esta observación empírica fue clave para darnos cuenta de que la fase debía ser preservada para obtener una reconstrucción fiel. Así nació el modo `fft` completo, que no solo corrige esa limitación sino que también convierte el sistema en un ejemplo práctico y tangible de conceptos vistos en teoría de procesamiento de señal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar → Empaquetar → Decodificar para FFT (gris y color)\n",
    "map_mode = \"fft\"\n",
    "for color in (\"gris\", \"color\"):\n",
    "    # 3.1.1 Carpetas de salida\n",
    "    frames_dir = f\"fotogramas/{map_mode}_{color}_fotogramas\"\n",
    "    export_dir = f\"exports/{map_mode}_{color}\"\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # CODIFICAR\n",
    "    enc = PixelSoundsEncoder(\n",
    "        audio_path=audio_path,\n",
    "        frames_dir=frames_dir,\n",
    "        export_dir=export_dir,\n",
    "        fps=fps,\n",
    "        color_mode=color,\n",
    "        map_mode=map_mode,\n",
    "        window_type=window_type\n",
    "    )\n",
    "    enc.generate_frames()\n",
    "\n",
    "    # EMPAQUETAR VÍDEO\n",
    "    video_file = enc.encode_video()\n",
    "    print(f\"[Notebook] Vídeo generado en: {video_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead31d1",
   "metadata": {},
   "source": [
    "### Visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55ae96",
   "metadata": {},
   "source": [
    "\n",
    "Los vídeos generados en modo **FFT** suelen ser muy grandes y a veces el notebook tarda o se “congela” al intentar renderizarlos inline. Por ello **no recomendamos** ejecutar la celda de abajo en el notebook si ves que no responde. \n",
    "\n",
    "> **Sugerencia**:  \n",
    "> - Abre el fichero MP4 directamente en **VLC** o **VSCode**  \n",
    "> - O puedes reproducirlos en un navegador externo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68df4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "\n",
    "for map_mode, color in [(\"fft\", \"gris\"), (\"fft\", \"color\")]:\n",
    "    video_file = os.path.join(\n",
    "        \"exports\",\n",
    "        f\"{map_mode}_{color}\",\n",
    "        f\"{base}_{map_mode}_{color}.mp4\"\n",
    "    )\n",
    "    print(f\"Vídeo FFT ({map_mode}_{color}): {video_file}\")\n",
    "    # DESCOMENTA la siguiente si quieres intentarlo (no lo recomendamos ) \n",
    "    # display(Video(video_file, embed=True, width=480, height=360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd806145",
   "metadata": {},
   "source": [
    "### Decodificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27086ca0",
   "metadata": {},
   "source": [
    "- **Lectura de magnitud y fase**  \n",
    "  - En modo color se toman los canales R y G.  \n",
    "  - En modo gris se extraen de las filas 0 (magnitud) y 1 (fase).\n",
    "\n",
    "- **Desnormalización de magnitud y fase**  \n",
    "  $$\n",
    "  \\text{mag} = \\frac{\\text{mag}_n}{255}, \\qquad \\text{phase} = \\frac{\\text{phase}_n}{255} \\times 2\\pi - \\pi\n",
    "  $$\n",
    "\n",
    "- **Reconstrucción del espectro complejo**  \n",
    "  $$\n",
    "  X = \\text{mag} \\cdot e^{j \\cdot \\text{phase}}\n",
    "  $$\n",
    "\n",
    "- **Transformada inversa y desfase de espectro**  \n",
    "  $$\n",
    "  X = \\text{ifftshift}(X), \\quad \\text{block} = \\text{Re}\\left( \\text{IFFT}(X) \\right)\n",
    "  $$\n",
    "\n",
    "- **Solapamiento y normalización**  \n",
    "  Se aplica overlap-add con ventana y se normaliza por los pesos acumulados, como en los otros modos.\n",
    "\n",
    "El modo `fft` permite recuperar tanto el contenido espectral como la fase del bloque, generando reconstrucciones más fieles pero a costa de mayor complejidad.\n",
    "\n",
    "> A diferencia de representaciones comunes como el espectrograma, que descartan la fase, este sistema la conserva y la reutiliza explícitamente durante la reconstrucción. El resultado es una señal mucho más fiel al original, sin los artefactos metálicos típicos de codificaciones espectrales incompletas.\n",
    "\n",
    "> Esta simetría entre codificación y decodificación no solo refuerza la robustez del sistema, sino que también proporciona una implementación práctica de lo discutido en clase sobre el papel crucial de la fase en la calidad perceptiva del sonido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e282d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_mode = \"fft\"\n",
    "base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "\n",
    "for color in (\"gris\", \"color\"):\n",
    "    # 1) Directorios y rutas\n",
    "    frames_dir = f\"fotogramas/{map_mode}_{color}_fotogramas\"\n",
    "    export_dir = f\"exports/{map_mode}_{color}\"\n",
    "    # Nombre del vídeo que acabamos de generar\n",
    "    video_file = os.path.join(export_dir, f\"{base_name}_{map_mode}_{color}.mp4\")\n",
    "    # Fichero WAV de salida\n",
    "    output_wav = os.path.join(export_dir, f\"recon_{base_name}_{map_mode}_{color}.wav\")\n",
    "    \n",
    "    # 2) Instanciar decoder\n",
    "    dec = PixelSoundsDecoder(\n",
    "        frames_dir=frames_dir, # Donde guardar los frames extraidos\n",
    "        output_wav=output_wav, # Video del que sacar los frames\n",
    "        map_mode=map_mode,     # Modo\n",
    "        window_type=window_type # Tipo de ventana\n",
    "    )\n",
    "    \n",
    "    # 3) Extraer frames desde el MP4\n",
    "    dec.extract_all_frames(video_file)\n",
    "    \n",
    "    # 4) Reconstruir el audio y guardarlo\n",
    "    audio_rec, fs_rec = dec.decode()\n",
    "    \n",
    "    # 5) Mostrar y reproducir inline\n",
    "    print(f\"[Notebook] Audio reconstruido ({map_mode}_{color}):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c7ae9",
   "metadata": {},
   "source": [
    "### 5. Modo FIR (`fir`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ab1fc",
   "metadata": {},
   "source": [
    "\n",
    "#### Codificación\n",
    "\n",
    "- **Filtrado en tres bandas**  \n",
    "  Se aplica un banco de filtros FIR al bloque:\n",
    "\n",
    "  $$\n",
    "  y_L = \\text{lfilter}(b_{\\text{low}}, 1, \\text{block}) \\\\\n",
    "  y_B = \\text{lfilter}(b_{\\text{band}}, 1, \\text{block}) \\\\\n",
    "  y_H = \\text{lfilter}(b_{\\text{high}}, 1, \\text{block})\n",
    "  $$\n",
    "\n",
    "- **Clipping y cuantificación a enteros de 8 bits con signo**  \n",
    "  Se limita cada banda al rango \\([-1, +1]\\) y se escala a 8 bits con signo:\n",
    "\n",
    "  $$\n",
    "  r = \\left\\lfloor y_L \\cdot 127 \\right\\rfloor, \\quad\n",
    "  g = \\left\\lfloor y_B \\cdot 127 \\right\\rfloor, \\quad\n",
    "  b = \\left\\lfloor y_H \\cdot 127 \\right\\rfloor\n",
    "  $$\n",
    "\n",
    "  Posteriormente se reinterpretan como `uint8` para almacenarlos en la imagen:\n",
    "\n",
    "  $$\n",
    "  r_8 = \\text{reinterpretar como uint8}(r), \\quad \\text{etc.}\n",
    "  $$\n",
    "\n",
    "- **Construcción de la imagen**  \n",
    "  - **Modo color**:  \n",
    "    Se construye un frame RGB directamente con los canales \\((r_8, g_8, b_8)\\).\n",
    "\n",
    "  - **Modo gris**:  \n",
    "    Se intercalan las tres bandas en las filas de la imagen:\n",
    "\n",
    "    $$\n",
    "    \\text{img}_{3k} = r_8, \\quad \\text{img}_{3k+1} = g_8, \\quad \\text{img}_{3k+2} = b_8\n",
    "    $$\n",
    "\n",
    "\n",
    "> Esta codificación propone una lectura creativa del espectro dividiéndolo en tres bandas significativas y mapeándolas directamente a los colores primarios. La baja frecuencia domina el canal rojo, la media el verde, y la alta el azul, lo que permite que cada frame del vídeo adquiera un color característico según la energía distribuida en el tiempo.\n",
    "\n",
    "> Aunque no conserva la fase ni permite una reconstrucción exacta, este modo tiene una gran fuerza expresiva y es especialmente útil para identificar patrones rítmicos o cambios tímbricos en la señal. El resultado es una representación visual directa e intuitiva, donde incluso una persona sin formación técnica puede \"ver\" cuándo un sonido es grave, agudo o ruidoso.\n",
    "\n",
    "> En este sentido, el modo `fir` ejemplifica cómo una idea técnica puede convertirse en una herramienta de análisis perceptual, o incluso en una forma de visualización artística del sonido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Generar → Empaquetar → Decodificar para FFT (gris y color)\n",
    "map_mode = \"fir\"\n",
    "for color in (\"gris\", \"color\"):\n",
    "    # 3.1.1 Carpetas de salida\n",
    "    frames_dir = f\"fotogramas/{map_mode}_{color}_fotogramas\"\n",
    "    export_dir = f\"exports/{map_mode}_{color}\"\n",
    "    os.makedirs(frames_dir, exist_ok=True)\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # 3.1.2 CODIFICAR\n",
    "    enc = PixelSoundsEncoder(\n",
    "        audio_path=audio_path,\n",
    "        frames_dir=frames_dir,\n",
    "        export_dir=export_dir,\n",
    "        fps=fps,\n",
    "        color_mode=color,\n",
    "        map_mode=map_mode,\n",
    "        window_type=window_type\n",
    "    )\n",
    "    enc.generate_frames()\n",
    "\n",
    "    # 3.1.3 EMPAQUETAR VÍDEO\n",
    "    video_file = enc.encode_video()\n",
    "    print(f\"[Notebook] Vídeo generado en: {video_file}\")\n",
    "    display(Video(video_file, embed=True, width=480, height=360))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f37694",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Decodificación\n",
    "\n",
    "- **Extracción de las bandas**  \n",
    "  - En **modo color**:\n",
    "\n",
    "    $$\n",
    "    y_L = \\text{reinterpretar como int8}(R) / 127 \\\\\n",
    "    y_B = \\text{reinterpretar como int8}(G) / 127 \\\\\n",
    "    y_H = \\text{reinterpretar como int8}(B) / 127\n",
    "    $$\n",
    "\n",
    "  - En **modo gris**:\n",
    "\n",
    "    $$\n",
    "    y_L = \\text{reinterpretar como int8}(\\text{fila }0::3) / 127 \\\\\n",
    "    y_B = \\text{reinterpretar como int8}(\\text{fila }1::3) / 127 \\\\\n",
    "    y_H = \\text{reinterpretar como int8}(\\text{fila }2::3) / 127\n",
    "    $$\n",
    "\n",
    "- **Suma de las bandas**  \n",
    "  Se combinan las tres bandas para reconstruir el bloque:\n",
    "\n",
    "  $$\n",
    "  \\text{block} = y_L + y_B + y_H\n",
    "  $$\n",
    "\n",
    "- **Solapamiento y reconstrucción final**  \n",
    "  Se hace overlap-add y normalización con la ventana, como en los otros modos.\n",
    "\n",
    "  Este modo es útil para representar la energía en distintas bandas del espectro, con menor fidelidad que `fft` pero menor tamaño y buena separación frecuencial.\n",
    "\n",
    "> Este modo mantiene la simetría con la codificación, reinterpretando cada canal como un valor con signo y escalando de vuelta a su rango original. Al sumar las tres bandas recuperadas se obtiene una estimación del bloque original, lo que permite reconstruir la forma general del sonido sin necesidad de conservar fase.\n",
    "\n",
    "> Aunque no ofrece una fidelidad espectral tan alta como el modo `fft`, su bajo coste computacional y su claridad visual lo convierten en una opción especialmente adecuada para analizar estructuras temporales, acentos o texturas sonoras. Es un ejemplo de cómo una codificación perceptiva —más que matemática— puede ser útil en tareas de análisis, educación musical o visualización artística del sonido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_mode = \"fir\"\n",
    "base_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "\n",
    "for color in (\"gris\", \"color\"):\n",
    "    # 1) Directorios y rutas\n",
    "    frames_dir = f\"fotogramas/{map_mode}_{color}_fotogramas\"\n",
    "    export_dir = f\"exports/{map_mode}_{color}\"\n",
    "    # Nombre del vídeo que acabamos de generar\n",
    "    video_file = os.path.join(export_dir, f\"{base_name}_{map_mode}_{color}.mp4\")\n",
    "    # Fichero WAV de salida\n",
    "    output_wav = os.path.join(export_dir, f\"recon_{base_name}_{map_mode}_{color}.wav\")\n",
    "    \n",
    "    # 2) Instanciar decoder\n",
    "    dec = PixelSoundsDecoder(\n",
    "        frames_dir=frames_dir, # Donde guardar los frames extraidos\n",
    "        output_wav=output_wav, # Video del que sacar los frames\n",
    "        map_mode=map_mode,     # Modo\n",
    "        window_type=window_type # Tipo de ventana\n",
    "    )\n",
    "    \n",
    "    # 3) Extraer frames desde el MP4\n",
    "    dec.extract_all_frames(video_file)\n",
    "    \n",
    "    # 4) Reconstruir el audio y guardarlo\n",
    "    audio_rec, fs_rec = dec.decode()\n",
    "    \n",
    "    # 5) Mostrar y reproducir inline\n",
    "    print(f\"[Notebook] Audio reconstruido ({map_mode}_{color}):\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
