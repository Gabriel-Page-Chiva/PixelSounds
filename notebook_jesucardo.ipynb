{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3c3128",
   "metadata": {},
   "source": [
    "# PIXELSOUNDS\n",
    "## Codificador de audio a vídeo\n",
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8999096",
   "metadata": {},
   "source": [
    "En este caso, y siguiendo en la línea de los anteriores conversores propuestos en el proyecto PixelSounds, vamos a tratar de convertir audio a vídeo. Sin embargo, no va a ser cualquier tipo de vídeo. Vamos a tratar de generar un vídeo que nos permita visualizar en una primera vista algunas características del audio que hay detrás. Para ello vamos a extraer algunos descriptores del audio (los cuales especificaremos más adelante) y vamos a transformarlos en algo visual. De esta forma, iremos generando fotogramas y mediante la libreria `ffmpeg` los uniremos para crear un video. En este cuaderno vamos a ir desgranando una por una las transformaciones que haremos para visualizar los distintos descriptores. Después uniremos todas las funciones para generar un fotograma. Tras aprender a generar un fotograma, podremos generar muchos para crear el vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee9900",
   "metadata": {},
   "source": [
    "### Librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b1bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vozyaudio as vz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "from vozyaudio import lee_audio, envolvente, track_pitch, espectro\n",
    "from scipy.signal import resample, correlate, find_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d627e05",
   "metadata": {},
   "source": [
    "### Los primeros pasos\n",
    "Antes de comenzar a hacer nada, vamos a definir algunos parámetros básicos que vamos a utilizar más adelante. La configuración del codificador consta de los siguientes parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb2d03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURACIÓN\n",
    "AUDIO_PATH = 'audios/music.wav' # Ruta del audio que vamos a usar\n",
    "FPS = 25 # Número de fotogramas por segundo del vídeo resultante\n",
    "FRAME_FOLDER = 'fotogramas' # Ruta de la carpeta donde iremos guardando los fotogramas\n",
    "VIDEO_PATH = 'output.mp4' # Nombre del video\n",
    "N_BARRAS = 60  # Número de barras del espectro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b76af",
   "metadata": {},
   "source": [
    "Después de esto, cargaremos el audio y extraeremos su duración, número de muestras, y muestras por fotograma. Además, obtendremos el número de frames que ha de tener el vídeo resultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8161cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar audio\n",
    "fs, x = lee_audio(AUDIO_PATH)\n",
    "x = x.astype(np.float32)\n",
    "dur = len(x) / fs\n",
    "n_frames = int(FPS * dur)\n",
    "samples_per_frame = int(fs / FPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83301fad",
   "metadata": {},
   "source": [
    "Tras hacer todo esto ya podemos empezar a generar las primeras componentes del fotograma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecc4b1",
   "metadata": {},
   "source": [
    "### Círculo que se mueve con el pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead76ada",
   "metadata": {},
   "source": [
    "Lo primero que vamos a agregar al fotograma es un **circulo que se mueve de arriba a abajo con el pitch y cambia de tamaño con la envolvente**. Para este paso, usaremos las funciones `envolvente` y `track_pitch` del módulo `vozyaudio` para extraer la envolvente y estimar el pitch del audio. Despues normalizaremos ambos descriptores y los redimensionaremos para que se ajusten al número de frames finales del video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos los descriptores de envolvente y estimación de pitch\n",
    "env = envolvente(x, fs=fs) # Extraer envolvente\n",
    "pitch = track_pitch(x, fs) # Estimar pitch\n",
    "pitch = np.nan_to_num(pitch)  # Reemplaza NaNs por 0\n",
    "\n",
    "def normalizar(v):\n",
    "    \"\"\" Normaliza un vector al rango [0, 1].\n",
    "    Entrada:\n",
    "        v (numpy.ndarray): Vector de valores (por ejemplo, envolvente, pitch, espectro, etc.)\n",
    "    Salida:\n",
    "        v_norm (numpy.ndarray): Vector normalizado en el rango [0, 1]\n",
    "    \"\"\"\n",
    "    return (v - np.min(v)) / (np.max(v) - np.min(v) + 1e-9)\n",
    "\n",
    "env = normalizar(env) # Normalizar ambos arrays\n",
    "pitch = normalizar(pitch)\n",
    "\n",
    "# Redimensionar pitch y envolvente al número de frames\n",
    "pitch_frame = np.interp(np.linspace(0, len(pitch), n_frames), np.arange(len(pitch)), pitch)\n",
    "env_frame = np.interp(np.linspace(0, len(env), n_frames), np.arange(len(env)), env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741604c9",
   "metadata": {},
   "source": [
    "Con estas variables ya tenemos todo lo necesario para generar el primer componente que variará durante el vídeo. Lo siguiente que debemos hacer es definir una función que dibujo el circulo en pantalla variando segúne estos valores. A esta función la llamaremos `dibujar_particula`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bbcd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dibujar_particula(ax, pitch, env):\n",
    "    \"\"\" Dibuja una partícula que se mueve según el pitch y cambia de tamaño según la envolvente del audio\n",
    "    Entrada:\n",
    "        ax (matplotlib.axes.Axes): Objeto de ejes sobre el que se dibuja la partícula\n",
    "        pitch (numpy.ndarray) : Array con los valores de estimación del pitch del audio\n",
    "        env (numpy.ndarray) : Array con los valores de la envolvente del audio\n",
    "    Salida:\n",
    "        None: La función solo dibuja en la figura y no devuelve nada\n",
    "    \"\"\"\n",
    "    y_pos = pitch\n",
    "    size = 100 + env * 300\n",
    "    color = (1.0, env, pitch)\n",
    "    ax.scatter(0.5, y_pos, s=size, c=[color], alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a76a4",
   "metadata": {},
   "source": [
    "Para visualizar que el resultado es el que esperamos tenemos que definir un esqueleto básico de generación de frames. Este esqueleto lo iremos ampliando a medida que vayamos añadiendo más componentes al fotograma. Cuando tengamos todos los fotogramas generados los uniremos mediante la función `crear_video`  para ver el resultado. Esta función hace uso de `subprocess` y de `ffmpeg` para crear el video de manera rápida simplemente pasándole el archivo `generarVideo.bat` que se encargará de ejecutar los comandos ffmpeg. La usaremos mucho a lo largo del cuaderno. De momento, probemos a generar los fotogramas con solo esta componente y ver el video resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando frames...\n",
      "Frames generados.\n"
     ]
    }
   ],
   "source": [
    "# Generar frames\n",
    "print(\"Generando frames...\")\n",
    "for i in range(n_frames):\n",
    "    porcentaje = (i / n_frames) * 100\n",
    "    print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.set_facecolor((0, 0, 0))  # Fondo negro\n",
    "\n",
    "    # Visual: Círculo que sube/baja con pitch y cambia tamaño con envolvente\n",
    "    dibujar_particula(ax, pitch_frame[i], env_frame[i])\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Frames generados.\")\n",
    "\n",
    "try:\n",
    "    subprocess.run(['generarVideo.bat', AUDIO_PATH], check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error al ejecutar generarVideo.bat:\", e)\n",
    "finally:\n",
    "    # shutil.rmtree('fotogramas/')\n",
    "    print('Procesado terminado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab720dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(v):\n",
    "    \"\"\" Normaliza un vector al rango [0, 1].\n",
    "    Entrada:\n",
    "        v (numpy.ndarray): Vector de valores (por ejemplo, envolvente, pitch, espectro, etc.)\n",
    "    Salida:\n",
    "        v_norm (numpy.ndarray): Vector normalizado en el rango [0, 1]\n",
    "    \"\"\"\n",
    "    return (v - np.min(v)) / (np.max(v) - np.min(v) + 1e-9)\n",
    "\n",
    "\n",
    "def autocorrelacion(x_frame):\n",
    "    \"\"\" Calcula la autocorrelación normalizada de una ventana de señal de audio.\n",
    "    Entrada:\n",
    "        x_frame (numpy.ndarray): Fragmento de señal de audio (ventana temporal)\n",
    "    Salida:\n",
    "        corr_norm (numpy.ndarray): Autocorrelación normalizada desde el retardo cero hacia adelante\n",
    "    \"\"\"\n",
    "    x_frame = x_frame - np.mean(x_frame)\n",
    "    corr = correlate(x_frame, x_frame, mode='full')\n",
    "    mid = len(corr) // 2\n",
    "    return corr[mid:] / np.max(np.abs(corr) + 1e-9)\n",
    "\n",
    "\n",
    "def detectar_ritmo(x_frame, fs, fmin=1.5, fmax=8):\n",
    "    \"\"\" Estima el periodo rítmico de un fragmento de audio mediante autocorrelación.\n",
    "    Entrada:\n",
    "        x_frame (numpy.ndarray): Fragmento de señal de audio (ventana temporal)\n",
    "        fs (int): Frecuencia de muestreo del audio\n",
    "        fmin (float): Frecuencia mínima esperada del ritmo (en Hz)\n",
    "        fmax (float): Frecuencia máxima esperada del ritmo (en Hz)\n",
    "    Salida:\n",
    "        periodo_seg (float): Periodo estimado del ritmo en segundos\n",
    "        corr (numpy.ndarray): Autocorrelación normalizada del fragmento de audio\n",
    "    \"\"\"\n",
    "    corr = autocorrelacion(x_frame)\n",
    "    min_lag = int(fs / fmax)\n",
    "    max_lag = int(fs / fmin)\n",
    "    if max_lag >= len(corr): max_lag = len(corr) - 1\n",
    "    if min_lag >= max_lag: return 0.5, corr  # Valor por defecto\n",
    "    pico = np.argmax(corr[min_lag:max_lag]) + min_lag\n",
    "    periodo_seg = pico / fs\n",
    "    return periodo_seg, corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Cargar audio ===\n",
    "fs, x = lee_audio(AUDIO_PATH)\n",
    "x = x.astype(np.float32)\n",
    "dur = len(x) / fs\n",
    "n_frames = int(FPS * dur)\n",
    "samples_per_frame = int(fs / FPS)\n",
    "\n",
    "# === 2. Descriptores ===\n",
    "env = envolvente(x, fs=fs)\n",
    "pitch = track_pitch(x, fs)\n",
    "pitch = np.nan_to_num(pitch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846428fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Autocorrelación sobre la envolvente\n",
    "env_smooth = envolvente(x, fs=fs, tr=0.1)  # más estable\n",
    "corr_env = autocorrelacion(env_smooth)\n",
    "\n",
    "# 2. Estimar el tempo global\n",
    "min_lag = int(fs / 5)    # máx 5 Hz = 300 BPM\n",
    "max_lag = int(fs / 1.5)  # mín 1.5 Hz = 90 BPM\n",
    "lag_beat = np.argmax(corr_env[min_lag:max_lag]) + min_lag\n",
    "periodo_muestras = lag_beat\n",
    "\n",
    "# 3. Encontrar los picos en la envolvente\n",
    "peaks, _ = find_peaks(env_smooth, distance=periodo_muestras * 0.8)\n",
    "\n",
    "# Convertir los picos (en muestras) a tiempos (en segundos) y luego a frames\n",
    "beat_times = peaks / fs\n",
    "beat_frames = (beat_times * FPS).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = normalizar(env)\n",
    "pitch = normalizar(pitch)\n",
    "\n",
    "# Redimensionar descriptores al número de frames\n",
    "env_frame = np.interp(np.linspace(0, len(env), n_frames), np.arange(len(env)), env)\n",
    "pitch_frame = np.interp(np.linspace(0, len(pitch), n_frames), np.arange(len(pitch)), pitch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_beat(frame_index, beat_frames, tolerancia=2):\n",
    "    \"\"\" Determina si un frame está dentro de los limites de un beat detectado.\n",
    "    Entrada:\n",
    "        frame_index (int): Índice del frame actual en el video\n",
    "        beat_frames (list of int): Lista de frames donde se detectaron beats\n",
    "        tolerancia (int): Número de frames de margen alrededor de cada beat\n",
    "    Salida:\n",
    "        es_beat (bool): True si el frame está cerca de un beat, False en caso contrario\n",
    "    \"\"\"\n",
    "    return any(abs(frame_index - bf) <= tolerancia for bf in beat_frames)\n",
    "\n",
    "\n",
    "def dibujar_flash(ax):\n",
    "    \"\"\" Dibuja un flash visual en el centro del frame, usado para resaltar beats detectados.\n",
    "    Entrada:\n",
    "        ax (matplotlib.axes.Axes): Objeto de ejes sobre el que se dibuja el flash\n",
    "    Salida:\n",
    "        None: El flash se dibuja directamente sobre la figura\n",
    "    \"\"\"\n",
    "    ax.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "\n",
    "\n",
    "def obtener_frame_audio(x, i, samples_per_frame):\n",
    "    \"\"\" Extrae un fragmento de audio correspondiente a un frame de video.\n",
    "    Entrada:\n",
    "        x (numpy.ndarray): Señal de audio completa\n",
    "        i (int): Índice del frame actual\n",
    "        samples_per_frame (int): Número de muestras de audio por frame\n",
    "    Salida:\n",
    "        x_frame (numpy.ndarray): Fragmento de audio correspondiente al frame i\n",
    "    \"\"\"\n",
    "    start = i * samples_per_frame\n",
    "    end = min(len(x), start + samples_per_frame)\n",
    "    return x[start:end]\n",
    "\n",
    "\n",
    "def dibujar_circulo_ritmico(ax, t_actual, periodo):\n",
    "    \"\"\" Dibuja un círculo que pulsa rítmicamente en el centro del frame según un periodo dado.\n",
    "    Entrada:\n",
    "        ax (matplotlib.axes.Axes): Objeto de ejes sobre el que se dibuja el círculo\n",
    "        t_actual (float): Tiempo actual del video en segundos\n",
    "        periodo (float): Periodo rítmico estimado en segundos\n",
    "    Salida:\n",
    "        None: El círculo se dibuja directamente sobre la figura\n",
    "    \"\"\"\n",
    "    ritmo_osc = 0.5 * (1 + np.sin(2 * np.pi * t_actual / periodo))\n",
    "    color = (ritmo_osc, 0.2, 1 - ritmo_osc)\n",
    "    size = 300 * ritmo_osc + 20\n",
    "    ax.scatter(0.5, 0.5, s=size, c=[color], alpha=0.3)\n",
    "\n",
    "\n",
    "def dibujar_barras(ax, X_resampled, N_BARRAS):\n",
    "    \"\"\" Dibuja barras verticales que representan la energía en diferentes bandas de frecuencia.\n",
    "    Entrada:\n",
    "        ax (matplotlib.axes.Axes): Objeto de ejes sobre el que se dibujan las barras\n",
    "        X_resampled (numpy.ndarray): Vector con las amplitudes espectrales reescaladas a N_BARRAS bandas\n",
    "        N_BARRAS (int): Número total de barras a dibujar\n",
    "    Salida:\n",
    "        None: Las barras se dibujan directamente sobre la figura\n",
    "    \"\"\"\n",
    "    bar_width = 1 / N_BARRAS\n",
    "    for j in range(N_BARRAS):\n",
    "        height = X_resampled[j]\n",
    "        color = (0.1, 0.8 * height, 1.0)\n",
    "        ax.bar(j * bar_width, height, width=bar_width*0.8, color=color, align='edge')\n",
    "\n",
    "def dibujar_particula(ax, pitch, env):\n",
    "    y_pos = pitch\n",
    "    size = 100 + env * 300\n",
    "    color = (1.0, env, pitch)\n",
    "    ax.scatter(0.5, y_pos, s=size, c=[color], alpha=0.8)\n",
    "\n",
    "def finalizar_figura(fig, ax, path):\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_frames(x, fs, pitch_frame, env_frame, beat_frames, n_frames, samples_per_frame, FPS, N_BARRAS, FRAME_FOLDER):\n",
    "    print(\"Generando frames...\")\n",
    "    \n",
    "    # Crear carpeta\n",
    "    os.makedirs(FRAME_FOLDER, exist_ok=True)\n",
    "    \n",
    "    for i in range(n_frames):\n",
    "        porcentaje = (i / n_frames) * 100\n",
    "\n",
    "        print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.set_facecolor((0, 0, 0))\n",
    "        \n",
    "        if es_beat(i, beat_frames):\n",
    "            dibujar_flash(ax)\n",
    "        \n",
    "        x_frame = obtener_frame_audio(x, i, samples_per_frame)\n",
    "        t_actual = i / FPS\n",
    "        periodo, _ = detectar_ritmo(x_frame, fs)\n",
    "        dibujar_circulo_ritmico(ax, t_actual, periodo)\n",
    "\n",
    "        X, _ = espectro(x_frame, modo=1, fs=fs)\n",
    "        X_resampled = normalizar(resample(X, N_BARRAS))\n",
    "        dibujar_barras(ax, X_resampled, N_BARRAS)\n",
    "\n",
    "        dibujar_particula(ax, pitch_frame[i], env_frame[i])\n",
    "        \n",
    "        finalizar_figura(fig, ax, f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "    print(\"\\nFrames generados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Generar Frames ===\n",
    "print(\"Generando frames...\")\n",
    "for i in range(n_frames):\n",
    "    porcentaje = (i / n_frames) * 100\n",
    "    print(f\"\\rCompletado {porcentaje:.2f} %\", end=\"\", flush=True)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.set_facecolor((0, 0, 0))  # Fondo negro\n",
    "    \n",
    "    # Flash más visible en beat\n",
    "    if any(abs(i - bf) <= 2 for bf in beat_frames):  # mayor tolerancia\n",
    "        ax.scatter(0.5, 0.5, s=1500, c='cyan', alpha=0.9, edgecolors='none', marker='o')\n",
    "\n",
    "    # Halo pulsante animado en el centro tras beat\n",
    "    for bf in beat_frames:\n",
    "        frames_from_beat = i - bf\n",
    "        if 0 <= frames_from_beat <= 4:  # duración 4 frames\n",
    "            grow = 1 - frames_from_beat / 4\n",
    "            size = 2000 * grow\n",
    "            alpha = 0.8 * grow\n",
    "            ax.scatter(0.5, 0.5, s=size, c='magenta', alpha=alpha, edgecolors='none')\n",
    "\n",
    "    # ==== 3.1 Obtener trozo de señal actual ====\n",
    "    start = i * samples_per_frame\n",
    "    end = min(len(x), start + samples_per_frame)\n",
    "    x_frame = x[start:end]\n",
    "    \n",
    "    # Detectar ritmo\n",
    "    periodo, _ = detectar_ritmo(x_frame, fs)\n",
    "    t_actual = i / FPS\n",
    "    ritmo_osc = 0.5 * (1 + np.sin(2 * np.pi * t_actual / periodo))  # 0..1\n",
    "\n",
    "    # Efecto visual rítmico: círculo que late en el centro\n",
    "    ritmo_color = (ritmo_osc, 0.2, 1 - ritmo_osc)\n",
    "    ritmo_size = 300 * ritmo_osc + 20\n",
    "    ax.scatter(0.5, 0.5, s=ritmo_size, c=[ritmo_color], alpha=0.3)\n",
    "    \n",
    "    # ==== 3.2 Espectro (resample a N barras) ====\n",
    "    X, fa = espectro(x_frame, modo=1, fs=fs)\n",
    "    X_resampled = resample(X, N_BARRAS)\n",
    "    X_resampled = normalizar(X_resampled)\n",
    "\n",
    "    # ==== 3.3 Dibujar barras ====\n",
    "    bar_width = 1 / N_BARRAS\n",
    "    for j in range(N_BARRAS):\n",
    "        height = X_resampled[j]\n",
    "        ax.bar(j * bar_width, height, width=bar_width*0.8, color=(0.1, 0.8*height, 1.0), align='edge')\n",
    "\n",
    "    # ==== 3.4 Dibujar partícula ====\n",
    "    y_pos = pitch_frame[i]\n",
    "    size = 100 + env_frame[i] * 300\n",
    "    color = (1.0, env_frame[i], pitch_frame[i])\n",
    "    ax.scatter(0.5, y_pos, s=size, c=[color], alpha=0.8)\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FRAME_FOLDER}/frame_{i:04d}.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"Frames generados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a596a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar Video\n",
    "try:\n",
    "    subprocess.run(['generarVideo.bat', AUDIO_PATH], check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error al ejecutar generarVideo.bat:\", e)\n",
    "finally:\n",
    "    # shutil.rmtree('fotogramas/')\n",
    "\n",
    "    print('Procesado terminado.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Voz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
